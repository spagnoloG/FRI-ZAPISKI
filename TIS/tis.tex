\documentclass{article}
\usepackage[margin=0.15cm]{geometry}
\usepackage{amsmath}
\usepackage{multicol}

\setlength{\columnseprule}{0.5pt}

\begin{document}

\begin{center}
    {\small TIS/FRI \par}
\end{center}

\begin{multicols}{3}

\section{Osnove}

\subsection{Ponovitev logaritmov}
\begin{small}
    \begin{itemize} 
        \item $log_a x = \dfrac{log_b x}{log_b a}$
        \item $log_b(\dfrac{x}{y}) = log_b x - log_b y$
        \item $x = b^y \implies log_b x = y$
        \item $log_2 x = log x$
        \item $0log0 = 0$
    \end{itemize}
\end{small}

\subsection{Entropija}
je povprecje vseh lastnih informacij:
\begin{center}
    \begin{math}
        H(X) = \sum_{i=1}^{n} p_i I_i = -\sum_{i=1}^{n} p_i log p_i
    \end{math}
\end{center}
Lastnosti: je zvezna, simetricna funckija (vrsni red $p_i$ ni pomemben, sestevanje je komutativno). Je vedno vecja
od 0 ($p_i \geq 0 \rightarrow -p_i \log p_i \geq 0 \rightarrow H(X) \geq 0$) in navzgor omejena z $\log n$.\\
Ce sta dogodka \textbf{neodvisna} velja aditivnost: $H(X, Y) = H(X) + H(Y)$.\\
Vec zaporednih dogodkov neodvisnega vira: $X^l = X \times \dots \times X \rightarrow H(X^l) = lH(X)$.

\section{Kodi}

\subsection{Uvod}
\textbf{Kod} sestavljajo \textit{kodne zamenjave}, ki so sestavljene iz znakov
\textbf{kodne abecede}. Stevilo znakov v kodni abecedi oznacujemo z \textbf{r}.\\
Ce so $\{p_1, \dots, p_n\}$ verjetnosti znakov $\{s_1, \dots, s_n\}$ osnovnega sporocila in $\{l_1, \dots, l_n\}$
dolzine prejetih kodnih zmanjav, je povprecna dolzina kodne zamenjave
\begin{center}
    \begin{math}
        L = \sum_{i=1}^n p_i l_i
    \end{math}
\end{center}

\subsection{Tipi kodov}
\begin{itemize}
    \item \textbf{optimalen} - ce ima najmanjso mozno dolzino kodnih zamenjav
    \item \textbf{idealen} - ce je povprecna dolzina kodnih zamenjav enaka entropiji
    \item \textbf{enakomeren} - ce je dolzina vseh kodnih zamenjav enaka
    \item \textbf{enoznacen} - ce lahko poljuben niz znakov dekodiramo na en sam nacin
    \item \textbf{trenuten} - ce lahko osnovni znak dekodiramo takoj, ko sprejmemo celotno kodno zamenjavo
\end{itemize}

\subsection{Kraftova neenakost}
Za dolzine kodnih zamenjav $\{l_1, \dots, l_n\}$ in \textit{r} znaki kodne abecede
obstaja trenutni kod, iff
\begin{center}
    \begin{math}
        \sum_{i=1}^n r^{-li} \leq 1
    \end{math}
\end{center}

\subsection{Povp. dolzina, ucinkovitost}
Najkrajse kodne zamenjave imamo, ce velja:
\begin{center}
    \begin{math}
        H_r(X) = L \rightarrow l_i = \lceil - \log_r p_i \rceil
    \end{math}
\end{center}
Ucinkovitost koda:
\begin{center}
    \begin{math}
        \eta = \dfrac{H(X)}{L \log_r}, \eta \in [0, 1]
    \end{math}
\end{center}
Kod je \textbf{gospodaren}, ce je $L$ znotraj:
\begin{center}
    \begin{math}
        H_r(X) \leq L < H_r(X) + 1
    \end{math}
\end{center}
kjer je $H_r(X)$:
\begin{center}
    \begin{math}
        H_r(X) = -\sum^{n}_{i=1} \dfrac{\log_{p_i}}{\log_r} = \dfrac{H(X)}{\log_r}
    \end{math}
\end{center}

\subsection{Shannonov prvi teorem}
Za nize neodvisnih znakov dozline \texit{n} obstajajo kodi,
za katere velja:
\begin{center}
    \begin{math}
        \lim_{n \rightarrow \infty} \dfrac{L_n}{n} = H(X)
    \end{math}
\end{center}
pri cemer je $H(X)$ entropija vira $X$.\\
Postopek kodiranja po Shannonu:
\begin{enumerate}
    \item znake razvrstimo po padajocih verjetnostih
    \item dolocimo stevilo znakov v vsaki kodni zamenjavi ($l_k$)
    \item za vse simbole izracunamo komulativne verjetnosti ($P_k = \sum_{i=1}^{k-1} p_i$)
    \item $P_k$ pretvorimo v bazo $r$. Kodno zamenjavo predstavlja prvih $l_k$ znakov necelega dela stevila
\end{enumerate}

\subsection{Fanojev kod}
Postopek kodiranja:
\begin{enumerate}
    \item znake razvrstimo po padajocih verjetnostih
    \item znake razdelimo v $r$ cim bolj enako verjetnih skupin
    \item Vsaki skupini priredimo enega od r znakov kodne abecede
    \item Deljenje ponovimo na vsaki od skupin. Postopek ponavljamo, dokler je mogoce
\end{enumerate}

\subsection{Huffmanov kod}
Huffmanov postopek kodiranja poteka od spodaj navzgor (Pri Fanoju je ravno obratno).
Pri huffmanovem kodu imamo dve fazi:
\begin{enumerate}
    \item Zdruzevanje
        \begin{enumerate}
            \item Posici r najmanj verjetnih znakov in jih zdruzi v sestavljeni znak, katerega verjetnost je vsota verjetnosti vseh znakov
            \item Preostale znake skupaj z novo sestavljenim znakom spet razvrsti
            \item Postopek ponavljaj dokler ne ostane samo r znakov
        \end{enumerate}
    \item Razdruzevanje
        \begin{enumerate}
            \item Vsakemu od preostalih znakov priredi po en znak kodirne abecede
            \item Vsak sestavljeni znak razstavi in mu priredi po en znak kodirne abecede
            \item Ko zmanjka sestavljenih znakov, je postopek zakljucen
        \end{enumerate}
\end{enumerate}
Pred kodiranjem, je vedno pametno preveriti, ce imamo zadostno stevilo znakov.
Veljati mora:
\begin{center}
    \begin{math}
        n = r + k(r-1), k \geq 0
    \end{math}
\end{center}
Ce imamo premalo znakov, jih po potrebi dodamo s verjetnostjo $p=0$.\\
Huffmanov kod lahko razsirimo tako, da vec osnovnih znakov zdruzujemo v sestavljene znake $\rightarrow$ bolj ucinkoviti kodi. Vendar
naletimo na nevarnost kombinacijske eksplozije.\\

\subsection{Aritmeticni kod}
Je \textbf{hiter} in \textbf{blizu optimalnemu} kodu, ter manj ucinkovit kot Huffmanov,
vendar se izogne kombinacijski eksploziji.
Vsak niz je predstavljen kot realno stevilo $0 \leq R < 1$, kar nam pove, da daljsi kot bo niz,
bolj natancno mora biti podano naravno stevilo $R$.\\
Postopek kodiranja(znakov ni potrebno razvrstiti):
\begin{enumerate}
    \item Zacnemo z intervalom $[0, 1)$
    \item Izbrani interval razdelimo na \texit{n} podintervalov, ki se ne prekrivajo. Sirine podintervalov ustrezajo
        verjetnostim znakov. Vsak podinterval predstavlja en znak
    \item Izberemo podinterval, ki ustreza iskanemu znaku
    \item Ce niz se ni koncan, izbrani podinterval ponovno razdelimo (bne 2.tocka)
    \item Niz lahko predstavimo s poljubnim realnim stevilom v zadnjem podintervalu
\end{enumerate}
Ko dobimo realni interval, ga samo se pretvorimo v binarnega s pomocjo klasicnega pretvarjanja
iz dec v bin stevilski sistem.

\subsection{Kod Lempel-Ziv (LZ77)} 
Stiskanje temelji na osnovi slovarja, tako, da ne potrebujemo
racunati verjetnosti za posamezne znake. \textbf{Kodirnik}
med branjem niza gradi slovar, in \textbf{dekodirnik} med branjem
kodnih zaamenjav rekonstruira slovar in znake.\\
\textbf{Kodiranje:} uporablja drseca okna, znaki se premikajo iz desne na levo.
Referenca je podana kot trojcek:
\begin{itemize}
    \item odmik - razdalja do zacetka enakega podniza v medpomnilniku
    \item dolzina enakega podniza
    \item naslednji znak
\end{itemize}
npr. (0, 0, A) - ni ujemanja, (4, 3, B) - 4 znake nazaj se ponovi 3 znakovni podniz, ki se nato zakljuci s B.\\
\textbf{dekodiranje:} sledimo kodnim zamenjavam

\subsection{Deflate}
Gre za predelan LZ77. Uporablja pare (odmik, dolzina). Ce ujemanja v kodni tabeli ni, zapise kar znak.
Uporablja dve kodni tabeli:
\begin{itemize}
    \item \textbf{tabela za znake in dolzine} - 285 simbolov
        (0-255 za osnovne znake, 256 konec bloka, 257-285 kodira dolzine)
        Kodne zamenjave brez dodatnih bitov, se zakodira s Huffmanom.
    \item \textbf{tabela odmikov}
\end{itemize}
Niz znakov se razdeli na bloke(64k)
vsak blok se kodira na enega od treh nacinov:
\begin{enumerate}
    \item \textbf{brez stiskanja} osnovni znaki se prepisejo
    \item \textbf{stiskanje s staticnim Huffmanom} (verjetnosti podane vnaprej), Huffmanovo drevo ni zakodirano v bloku
    \item \textbf{stiskanje s Huffmanom} izracunamo verjetnosti za vsak blok
\end{enumerate}
Glava posameznega bloka: 1bit - zadnji/ni zadnji blok + 2bita tip stiskanja + pri (3) se Huffmanovo drevo
Ker Huffmanovo drevo ni enolicno, uvedemo kanonicni Huffmanov kod. Postopek:
\begin{enumerate}
    \item znake razvrstimo najprej po dolzinah kodnih zamenjav in nato po abecedi
    \item prvi simbol ima same nicle
    \item vsakemu naslednjemu znaku dodelimo naslednjo binarno kodo (prejsnji + 1)
    \item ce je kodna zamenjava daljsa od binarne kode stevila, na koncu pripnemo niclo
    \item ponavjlaj (3) do konca
\end{enumerate}
Na taksen nacin dosezemo, da je potrebno kodirati samo dolzine kodnih zamenjav.

\subsection{Kod Lempel-Ziv (LZW)}
Osnovni slovar je podan in ga sporti doponjujemo. Alogritem za \textbf{kodiranje}:
\begin{verbatim}
N = ""
ponavljaj:
    preberi naslednji znak z
    ce je [N,z] v slovarju:
        N = [N, z]
    drugace:
        izpisi indeks k niza N
        dodaj [N, z] v slovar
        N = z
    izpisi indeks k niza N
\end{verbatim}
Algoritem za \textbf{dekodiranje}:
\begin{verbatim}
preberi indeks k
poisci niz N, ki ustreza indeksu k
izpisi N
L = N
ponavljaj:
    preberi indeks k
    ce je k v slovarju:
        poisci niz N
    drugace:
        N = [L, L(1)]
    izpisi N
    v slovar dodaj [L, N(1)]
    L = N
\end{verbatim}
LZW doseze optimalno stiskanje, pribliza se entropiji. % dodaj dokaz ce bo potrebno

\subsection{Verizno kodiranje ali RLE (run lenght encoding)} 
Namesto originalnih podatkov, sharnjujemo dolzino verige (fffeef $\rightarrow$ 3f2e1f).
Problemu, ko se podatki ne ponavljajo, se izognemo tako, da izvedemo kombinacijo direktnega kodiranja
in kodiranja RLE. % dodaj podrobnosti ce bo potrebno


\subsection{Stiskanje z izgubami}
S taksnim nacinom stiskanja lahko dosezemo veliko boljsa kompresijska razmerja, vendar izgubimo podatke.
Zato ga uporabljamo samo s formati, kjer se ne ukvarjamo z integriteto podatkov(MP3, MPEG, JPEG, $\dots$).
Postopki kodiranj znanih formatov:
\begin{itemize}
    \item \textbf{JPEG}
        \begin{enumerate}
            \item priprava slike $\rightarrow$ ker je svetlost bol pomembna, je barvna resolucija obicajno zmanjsana ($YC_RC_B$)
            \item aproksimacija vsake od treh komponent s 2D DCT
            \item kvantizacija $\rightarrow$  podatki ki bolj izstopajo so shranjeni manj natancno kot tisti ki so staticni
            \item kodiranje blokov s pomocjo entropije
            \item RLE cik-cak po sliki
            \item RLE kodiramo z Huffmanom ali Aritmenticnim kodom
        \end{enumerate}
    \item \textbf{MP3}
        \begin{enumerate}
            \item Modified DCT
            \item odstranitev za cloveka neslisnih frekvenc
            \item stereo, ce sta si L in R pretvorimo v mono
            \item Huffman na koncu
        \end{enumerate}
    \item \textbf{MPEG}
        \begin{enumerate}
            \item uvodno kodiranje $\rightarrow$ celotna slika JPEG
            \item nato pa kodiramo samo spremembe, ki so se zgodile v sliki JPEG s pomocjo vektorja premika. V
                primeru, da je prevec razlik, se ponovno kodira JPEG slika.
        \end{enumerate}
\end{itemize}

\subsection{Kompresijsko razmerje}
Izracunamo ga po formuli $\rightarrow$ stisnjeni binarni zapis C(M) / binarni zapis dokumenta (M):
\begin{center}
    \begin{math}
        R = C(M) / M
    \end{math}
\end{center}

\section{Kanali}

\subsection{Uvod}
Kanali so strukture, ki opisujejo medsebojno povezanost. Kanal prenasa informacijo o spemenljivki
$X$ do spremenljivke $Y$. Matematicno ga opisemo s \textbf{pogojnimi verjetnostmi}, ki povezujejo izhodne
verjetnosti z vhodom.

\subsection{Diskretni kanal brez spomina}
Povezuje diskretni nakljuicni spremenljivki, s koncno mnozico stanj $X=\{x_1, \dots, x_r\}$ in $Y=\{y_1, \dots, y_s\}$.
Obema nakljicnima spremenljivkama pripadajo tudi posamezne verjetnosti $P_X = \{p(x_1), \dots, p(x_r)\}$ in $P_Y = \{p(y_1), \dots, p(y_s)\}$. Kjer velja, da je vsota posamezne mnozice verjetnosti enaka 1. Kanal je definiran kot mnozica \textbf{pogojnih verjetnosti}
\begin{center}
    $p(y_j | x_i)$.
\end{center}
Pogojna verjetnost nam pove verjetnost za dogodek $y_j$ na izhodu iz kanala, ce je na vhodu v kanal dogodek $x_i$.
Brez spomina je zato, ker so pogojne vrjetnosti konstantne in torej neodvisne od prehodnih simbolov, velja
\begin{center}
    \begin{math}
        \sum_j p(y_j | x_i) = 1.
    \end{math} 
\end{center}
Kanal popolnoma podamo z $r \times s$ pogojnimi verjetnostmi.

\subsubsection{Binarni simetricni kanal (BSK)}
Gre za poseben primer diskretnega kanala brez spomina. Napaka kanala je $p$, saj se z verjetnostjo $p$ znak prenese
v napacnega.
\begin{center}
    \begin{math}
        P_k = 
                \begin{pmatrix}
                    1-p & p\\
                    p & 1-p 
                \end{pmatrix}
    \end{math}
\end{center}

\subsection{Pogojna entropija}
Pogojna entropija spremenljivke $Y$ pri znanem $X$ se zapise kot $H(Y|X)$.
Vzemimo, da se je zgodil dogodek $x_i \in X$. Entropija dogodka $Y$ je potem
\begin{center}
    \begin{math}
        H(Y|x_i) = - \sum_{j=1}^s p(y_j|x_i) \log(p(y_j| x_i)).
    \end{math}
\end{center}
Velja: $0 \leq H(Y| x_i)$.\\
Ce pa o dogodku X vemo le da se je zgodil, se lahko spomnemo na vis in uporabimo
\textbf{vezano verjetnost} dogodkov $X$ in $Y$, ki pravi:
\begin{center}
    \begin{math}
        p(x_i, y_i) = p(x_j|x_i)p(x_i)
    \end{math}
\end{center}
Za entropijo:
\begin{center}
    \begin{math}
        H(Y|X) = \sum_{i} p(x_i)H(Y|x_i)
    \end{math}
    \begin{math}
        = -\sum_{i=1}^r \sum_{j=1}^s p(x_i, y_i) \log p(y_i | x_i)
    \end{math}
\end{center}
Splosno velja: $0 \leq H(Y|X) \leq H(Y)$, ce poznamo spremenljivko $X$, se nedolocenost $Y$ ne more povecati (lahko se pomanjsa).

\subsection{Vezana entropija spremeljivk}
Vezana entropija nakljucnih spremenljivk $X$ in $Y$ je entropija para $(X, Y)$.
Pomembne zveze:
\begin{itemize}
    \item $p(x_i, y_j) = p(y_j| x_i)p(x_i)$,
    \item $\sum_j p(x_i, y_j) = p(x_i)$,
    \item $\sum_i p(x_i, y_j) = p(y_j)$,
    \item $\sum_{i,j} p(x_i, y_j) = 1$
\end{itemize}
Velja: $H(X, Y) = H(Y|X) + H(X)$, kar nam pove, da ce najprej izvemo, kaj se je zgodilo v dogodku $X$ in potem
dobimo se dodatne informacije od dogodku $Y$, vemo vse.
\subsubsection{Obrat kanala}
Ker velja tudi $H(X, Y) = H(X|Y) + H(Y)$, kanal lahko \textbf{obrnemo}
(sepravi vhod $Y$ in izhod $X$). Pri tem ne obracamo fizicnega procesa, ampak samo verjetnostno strukturo, ki definira kanal. \textbf{Pogoj:}  poznati moramo vhodne verjetnosti. Iz njih lahko dolocimo izhodne verjetnosti, ki jih lahko
uporabimo kot vhodne verjetnosti v obrnjeni kanal.
Lastnosti:
\begin{itemize}
    \item izracun izhodnih verjetnosti $p(y_j) = \sum_i p(y_j, x_i)p(x_i)$
    \item obratne pogojne vrjetnosti $p(x_i, y_j)= p(y_j|x_i)p(x_i) = p(x_i|y_j)p(y_j)$
\end{itemize}
Za sprejemnika sporocila so obratne pogojne verjetnosti zelo pomembne, saj z njimi lahko iz prejetih znakov doloci
verjetnost za vhodne znake.

\subsection{Medesebojna informacija}
Pove nam, koliko o eni spremenljivki izvemo iz druge spremenljivke,
definicija:
\begin{center}
    \begin{math}
        I(X;Y) = H(X) - H(X|Y)
    \end{math}
\end{center}
Lastnosti:
\begin{itemize}
    \item $I(X;Y) = H(X, Y) - H(X|Y) - H(Y|X)$
    \item $I(X;Y) = H(X) - H(X|Y)$
    \item $I(X;Y) = H(Y) - H(Y|X)$
    \item $I(X;Y) = H(X) + H(Y) - H(X, Y)$
    \item $I(X;Y) = \text{simetricna glede na } X \text{ in } Y$
    \item $I(X;Y) = -\sum_i\sum_j p(x_i, y_j) \log \frac{p(x_i)p(y_j)}{p(x_i, y_j)}$
    \item $I(X;Y) \geq 0$
    \item $I(X;X) = H(X)$
\end{itemize}

\subsection{Kapaciteta kanala}
Kapaciteta kanala je najvecja mozna medsebojna informacija, ki jo lahko prenesemo od vhoda na izhod.
\begin{center}
    $C =\underset{P(X)}{\max} I(X;Y)$
\end{center}
\subsubsection{Kapaciteta kanala BSK}
Lastnosti:
\begin{itemize}
    \item $C =\underset{P(X)}{\max} (H(Y) - H(Y|X))$
    \item $p(x_0) = \alpha, p(x_1) = 1 - \alpha$
    \item $I(X;Y) = H(Y) - H(Y|X) = \dots = H(Y) - H(p, 1-p)$
    \item $\frac{dI(X;Y)}{d \alpha} = 0$
    \item $H(Y) = 1 \Rightarrow C$ je max
    \item $C=I(X;Y) |_{\alpha = 1/2} = 1 - H(p, 1-p)$
\end{itemize}
\subsubsection{Kapacitata kanala BSK z brisanjem}
Definicija:
\begin{center}
    \begin{math}
        P_k = 
                \begin{pmatrix}
                    1-p & p & 0\\
                    0   & p & 1-p
                \end{pmatrix}
    \end{math}
\end{center}
Lastnosti:
\begin{itemize}
    \item $C = 1 - p$
    \item $p(x_0) = \alpha, p(x_1) = 1 - \alpha$
    \item $p(y_0) = (1-p)\alpha, p(y_1) = p, p(y_2) = (1-p)(1-\alpha)$
    \item $I(X;Y) = (1-p)H(\alpha, 1 - \alpha)$
    \item $\frac{dI(X;Y)}{d \alpha} = 0 \Rightarrow \alpha = 1/2$
\end{itemize}

\subsection{Shannonov drugi teorem}
Shannon je ugotovil, da nam zdruzevanje znakov v nize daje vec moznosti za doseganje zaneslijvega prenosa.\\
Naj bo $M$ stevilo razlicnih kodnih zamenjav, ki jih lahko oblikujemo z nizi dolzine $n$. Potem je \textbf{hitrost koda}
(prenosa)  definirana kot:
\begin{center}
    \begin{math}
        R = \frac{max H(X^n)}{n} = \frac{log M}{n}
    \end{math}
\end{center}
Hitrost je najvecja takrat, ko so dovoljene kodne zamenjave na vhodu enako verjetne. Shannonov teorem pravi, da je mozna
skoraj popolna komunikacija s hitrostjo, enako kapaciteti kanala.
\textbf{Teorem:}\\
\begin{center}
    Za \textbf{R $\leq$ C} obstaja kod, ki zagotavlja tako preverjanje informacije, da je verjetnost napake pri
    dekodiran  poljubno majhna. Za \textbf{R $>$ C} kod, ki bi omogocal preverjanje informacije s poljubno majhno
    verjetnostjo napake, \textbf{ne} obstaja.
\end{center}
Ce so znaki neodvisni, velja:
\begin{center}
    \begin{math}
        \log(H(X^n)) = n \log H(X) \Rightarrow R = H
    \end{math}
\end{center}
Za $R \leq \frac{\log 2^{nC}}{n} = C$ je mozno najti kodne zamenjave, ki omogocajo zanesljivo komunikacijo.

\section{Varno kodiranje}

\subsection{Uvod}
Omejili se bomo na enostavne linearne blocne kode za BSK. Dolzina bloka je $k$ znakov, abeceda je enaka abecedi
kanala, torej imamo $M=2^k$ blokov $x_1, \dots x_k$, $x_i \in \{0, 1\}$. Za potrebe varovanja dodamo se nekaj
varnostnih znakov, celotna dolzina vsake od M kodnih zamenjav je potem n. Namesto enega posljemo n enakih znakov.
Boljsi pristop pa je, da naredimo kode, kjer se povecujeta $n$ in $k$ hitreje od razilke $n -k$.

\subsection{Kontrolne vsote}
Varnost komunikacije povecamo tako, da dodamo nekaj bitov za preverjanje parnosti(paritetni biti).
Nastavljeni so tako, da je vsota bitov v aritmetiki po modulu 2 fiksna vrednost (0 ali 1).\\
\textit{Spomnimo se arsa}
\begin{center}
    \begin{tabular}{ |c|c|c| } 
        \hline
        +/-/XOR & 0 & 1 \\ 
        \hline
            0   & 0 & 1 \\ 
            1   & 1 & 0 \\ 
        \hline
    \end{tabular}
    AND $\sim \times$.\\
\end{center}

npr. $00|0, 01|1, 10|1, 11|1$(detektiramo samo eno napako).

\subsubsection{Pravokotni kodi}
Zapisemo ga v obliki pravokotnika, gledamo sodost po vrsticah in po stolpcih.

\begin{center}
    \begin{tabular}{ |c|cc| } 
        \hline
            1   & 0 & 1 \\ 
            0   & 1 & 1 \\ 
            \hline
            0   & 1 & 0 \\ 
        \hline
    \end{tabular}
\end{center}

\subsubsection{Trikotni kodi}
Vsota elementov v stolpcu in vrstici s paritetnim bitom vred mora biti soda. (ravno tako vsota paritetnih bitov)
\begin{center}
    \begin{tabular}{ ccc } 
        1   & 0 & \textbf{1} \\ 
        0   & \textbf{0} &  \\ 
        \textbf{1}   &  &  \\ 
    \end{tabular}
\end{center}

\subsection{Hammingova razdalja}
Hammingova razdalja med kodnima zamenjava nam pove stevilo znakov, na katerih se razlikujeta. Kodni zamenjavi sta enaki,
ce je razdalja 0, razdalja med razlicnimi kodi mora biti vsaj 1, drugace je kod \textbf{singularen}.
Razdalja je podana kot \textbf{minimalna} Hammingova razdalja med dvema kodnima zamenjavama.
Stevilo napak, ki jih kod zazna:
\begin{center}
    $d \geq e + 1 \rightarrow e_{max} = d-1$
\end{center}
\begin{center}
    $d \geq 2f + 1 \rightarrow f_{max} = \lfloor \frac{d-1}{2} \rfloor$
\end{center}

\subsubsection{Hammingov pogoj}
Za bloke dolzine $n$ lahko zgradimo $2^n$ razlicnih kodnih zamenjav. Ce zelimo zagotoviti odpornost na napake,
mora biti razdalja $d > 1$. Uporabni kodi imajo st. kodnih zamenjav $M = 2^k < 2^n$.
Hammingov pogoj: da bi lahko dekodirali vse kodne zamenjave, pri katerih je prislo do $e$ ali manj napak
mora veljati:
\begin{center}
    \begin{math}
        M \leq \frac{2^n}{\sum_{i=0}^e{n \choose i}}
    \end{math}
\end{center}

\subsection{Linearni blocni kodi}
% ostal prosojnice6b str 9

\end{multicols}
\end{document}


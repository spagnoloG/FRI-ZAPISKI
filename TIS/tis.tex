\documentclass{article}
\usepackage[margin=0.15cm]{geometry}
\usepackage{amsmath}
\usepackage{multicol}

\begin{document}

\begin{center}
	{\small TIS/FRI \par}
\end{center}


\begin{multicols}{2}
	\input{../VIS/ponovitev-analize.tex}

	\section{Osnove}

	\subsection{Ponovitev logaritmov}
	\begin{small}
		\begin{itemize}
			\item $log_a x = \dfrac{log_b x}{log_b a}$
			\item $log_b(\dfrac{x}{y}) = log_b x - log_b y$
			\item $x = b^y \implies log_b x = y$
			\item $log_2 x = log x$
			\item $0log0 = 0$
		\end{itemize}
	\end{small}

	\subsection{Bayesova formula}
	\begin{center}
		\begin{math}
			P(H_{i} | A) =
		\end{math}
		\begin{math}
			\dfrac{P(H_{i}) P(A | H_{i})}{P(A)}
		\end{math}=\\
		\begin{math}
			= \dfrac{P(H_{i}) P(A | H_{i})}{\sum_{k=1}^{n} P(H_{k}) P(A | H_{k})}
		\end{math}
	\end{center}

	\subsection{Lastna informacija}
	Opisuje dogodek, ki se je zgodil:
	\begin{center}
		\begin{math}
			I_i = \log_2(\frac{1}{p_i}) = - \log_2(p_i)
		\end{math}
	\end{center}

	\subsection{Entropija}
	je povprecje vseh lastnih informacij:
	\begin{center}
		\begin{math}
			H(X) = \sum_{i=1}^{n} p_i I_i = -\sum_{i=1}^{n} p_i log_2 p_i
		\end{math}
	\end{center}
	Lastnosti:
	\begin{enumerate}
		\item je zvezna, simetricna funckija (vrsni red $p_i$ ni pomemben, sestevanje je komutativno).
		\item $p_i \geq 0 \rightarrow -p_i \log_2 p_i \geq 0 \Rightarrow H(X) \geq 0$
		\item je navzgor omejena z $log_r n$.
	\end{enumerate}

	Ce sta dogodka \textbf{neodvisna} velja aditivnost: $H(X, Y) = H(X) + H(Y)$.\\
	Vec zaporednih dogodkov neodvisnega vira: $X^l = X \times \dots \times X \rightarrow H(X^l) = lH(X)$.

	\section{Kodi}

	\subsection{Uvod}
	\textbf{Kod} sestavljajo \textit{kodne zamenjave}, ki so sestavljene iz znakov
	\textbf{kodne abecede}. Stevilo znakov v kodni abecedi oznacujemo z \textbf{r}.\\
	Ce so $\{p_1, \dots, p_n\}$ verjetnosti znakov $\{s_1, \dots, s_n\}$ osnovnega sporocila in $\{l_1, \dots, l_n\}$
	dolzine prejetih kodnih zmanjav, je povprecna dolzina kodne zamenjave
	\begin{center}
		\begin{math}
			L = \sum_{i=1}^n p_i l_i
		\end{math}
	\end{center}

	\subsection{Tipi kodov} % dodaj slikco
	\begin{itemize}
		\item \textbf{optimalen} - ce ima najmanjso mozno dolzino kodnih zamenjav
		\item \textbf{idealen} - ce je povprecna dolzina kodnih zamenjav enaka entropiji
		\item \textbf{enakomeren} - ce je dolzina vseh kodnih zamenjav enaka
		\item \textbf{enoznacen/enolicen} - ce lahko poljuben niz znakov dekodiramo na en sam nacin
		\item \textbf{trenuten} - ce lahko osnovni znak dekodiramo takoj, ko sprejmemo celotno kodno zamenjavo
	\end{itemize}

	\subsection{Kraftova neenakost}
	Za dolzine kodnih zamenjav $\{l_1, \dots, l_n\}$ in \textit{r} znaki kodne abecede
	obstaja trenutni kod, iff
	\begin{center}
		\begin{math}
			\sum_{i=1}^n r^{-li} \leq 1
		\end{math}
	\end{center}
	Kraftova neenakost je \textbf{potrebni} pogoj za trenutnost koda.
	\textbf{Zadosten} pogoj za trenutnost koda: Nobena kodna zamenjava \textbf{ni} predpona drugi.
	\subsection{Povp. dolzina, ucinkovitost}
	Najkrajse kodne zamenjave imamo, ce velja:
	\begin{center}
		\begin{math}
			H_r(X) = L \rightarrow l_i = \lceil - \log_r p_i \rceil
		\end{math}
	\end{center}
	Ucinkovitost koda:
	\begin{center}
		\begin{math}
			\eta = \dfrac{H(X)}{L \log_2 r}, \eta \in [0, 1]
		\end{math}
	\end{center}
	Kod je \textbf{gospodaren}, ce je $L$ znotraj:
	\begin{center}
		\begin{math}
			H_r(X) \leq L < H_r(X) + 1
		\end{math}
	\end{center}
	Ce pa imamo vec neodvisnih znakov:

	kjer je $H_r(X)$:
	\begin{center}
		\begin{math}
			H_r(X) = -\sum^{n}_{i=1} \dfrac{\log_{p_i}}{\log_r} = \dfrac{H(X)}{\log_r}
		\end{math}
	\end{center}

	\subsection{Shannonov prvi teorem}
	Za nize neodvisnih znakov dozline \textit{n} obstajajo kodi,
	za katere velja:
	\begin{center}
		\begin{math}
			\lim_{n \rightarrow \infty} \dfrac{L_n}{n} = H(X)
		\end{math}
	\end{center}
	pri cemer je $H(X)$ entropija vira $X$. Neformalno: \textit{Vec znakov kot bomo
		zdruzevali, bolj se bomo priblizali entropiji.}

	$\Rightarrow$ Posledica prvega Shannonovega teorema, ce zdruzujemo znake v vecje, sestavljene
	znake potem velja:
	\begin{center}
		\begin{math}
			H_r(X^n) \leq L_n < H_r(X^n) + 1
		\end{math} \\
		\begin{math}
			nH_r(X) \leq L_n < nH_r(X) + 1
		\end{math} \\
		\begin{math}
			H_r(X) \leq \frac{L_n}{n} < H_r(X) + \frac{1}{n}
		\end{math}
	\end{center}

	\subsection{Shannonov kod}
	Postopek kodiranja po Shannonu:
	\begin{enumerate}
		\item znake razvrstimo po padajocih verjetnostih
		\item dolocimo stevilo znakov v vsaki kodni zamenjavi ($l_k$)
		\item za vse simbole izracunamo komulativne verjetnosti ($P_k = \sum_{i=1}^{k-1} p_i$)
		\item $P_k$ pretvorimo v bazo $r$. Kodno zamenjavo predstavlja prvih $l_k$ znakov necelega dela stevila
	\end{enumerate}

	\subsection{Fanojev kod}
	Postopek kodiranja:
	\begin{enumerate}
		\item znake razvrstimo po padajocih verjetnostih
		\item znake razdelimo v $r$ cim bolj enako verjetnih skupin
		\item Vsaki skupini priredimo enega od r znakov kodne abecede
		\item Deljenje ponovimo na vsaki od skupin. Postopek ponavljamo, dokler je mogoce
	\end{enumerate}
	$l_i$ dolocimo s pomocjo tabele.

	\subsection{Huffmanov kod}
	Huffmanov postopek kodiranja poteka od spodaj navzgor (Pri Fanoju je ravno obratno).
	Pri huffmanovem kodu imamo dve fazi:
	\begin{enumerate}
		\item Zdruzevanje
		      \begin{enumerate}
			      \item Posici r najmanj verjetnih znakov in jih zdruzi v sestavljeni znak, katerega verjetnost je vsota verjetnosti vseh znakov
			      \item Preostale znake skupaj z novo sestavljenim znakom spet razvrsti
			      \item Postopek ponavljaj dokler ne ostane samo r znakov
		      \end{enumerate}
		\item Razdruzevanje
		      \begin{enumerate}
			      \item Vsakemu od preostalih znakov priredi po en znak kodirne abecede
			      \item Vsak sestavljeni znak razstavi in mu priredi po en znak kodirne abecede
			      \item Ko zmanjka sestavljenih znakov, je postopek zakljucen
		      \end{enumerate}
	\end{enumerate}
	Pred kodiranjem, je vedno pametno preveriti, ce imamo zadostno stevilo znakov.
	Veljati mora:
	\begin{center}
		\begin{math}
			n = r + k(r-1), k \geq 0
		\end{math}
	\end{center}
	Ce imamo premalo znakov, jih po potrebi dodamo s verjetnostjo $p=0$.\\
	Huffmanov kod lahko razsirimo tako, da vec osnovnih znakov zdruzujemo v sestavljene znake $\rightarrow$ bolj ucinkoviti kodi. Vendar
	naletimo na nevarnost kombinacijske eksplozije. $l_i$ dolocimo s pomocjo tabele.

	\subsection{Aritmeticni kod}
	Je \textbf{hiter} in \textbf{blizu optimalnemu} kodu, ter manj ucinkovit kot Huffmanov,
	vendar se izogne kombinacijski  eksploziji. Aritmeticni kod ni \textbf{gospodaren} saj zanj velja:
	\begin{center}
		\begin{math}
			\mathcal{H}(X) \leq L_n \leq \mathcal{H}(X) + 2
		\end{math}
	\end{center}
	Sepravi se nam lahko zgodi, da pri kodiranju niza uporabimo dva znaka prevec.

	Vsak niz je predstavljen kot realno stevilo $0 \leq R < 1$, kar nam pove, da daljsi kot bo niz,
	bolj natancno mora biti podano naravno stevilo $R$.\\
	Postopek kodiranja(znakov ni potrebno razvrstiti):
	\begin{enumerate}
		\item Zacnemo z intervalom $[0, 1)$
		\item Izbrani interval razdelimo na \textit{n} podintervalov, ki se ne prekrivajo. Sirine podintervalov ustrezajo
		      verjetnostim znakov. Vsak podinterval predstavlja en znak
		\item Izberemo podinterval, ki ustreza iskanemu znaku
		\item Ce niz se ni koncan, izbrani podinterval ponovno razdelimo (bne 2.tocka)
		\item Niz lahko predstavimo s poljubnim realnim stevilom v zadnjem podintervalu
	\end{enumerate}
	Ko dobimo realni interval, ga samo se pretvorimo v binarnega s pomocjo klasicnega pretvarjanja
	iz dec v bin stevilski sistem.

	\subsubsection{Dekodiranje}
	Recimo, da dobimo k.z. 0101. Potem sta spodnji in zgornji meji izracunani na
	naslednji nacin\\
	\begin{math}
		\text{sp. meja: } 0.0101 \rightarrow 2^{-2} + 2^{-4}
	\end{math}\\
	\begin{math}
		\text{zg. meja: } 0.0110 \rightarrow 2^{-2} + 2^{-3} \text{(sp. meja + 1)}
	\end{math}
	Nato binarno razdeljujemo mejo. Ustavimo se ko zapolnimo eno izmed mej.

	\subsection{Kod Lempel-Ziv (LZ77)}
	Stiskanje temelji na osnovi slovarja, tako, da ne potrebujemo
	racunati verjetnosti za posamezne znake. \textbf{Kodirnik}
	med branjem niza gradi slovar, in \textbf{dekodirnik} med branjem
	kodnih zamenjav rekonstruira slovar in znake.\\
	\textbf{Kodiranje:} uporablja drseca okna, znaki se premikajo iz desne na levo.
	Referenca je podana kot trojcek:
	\begin{itemize}
		\item odmik - razdalja do zacetka enakega podniza v medpomnilniku
		\item dolzina enakega podniza
		\item naslednji znak
	\end{itemize}
	npr. (0, 0, A) - ni ujemanja, (4, 3, B) - 4 znake nazaj se ponovi 3 znakovni podniz, ki se nato zakljuci s B.\\
	\textbf{dekodiranje:} sledimo kodnim zamenjavam

	\subsection{Deflate}
	Gre za predelan LZ77. Uporablja pare (odmik, dolzina). Ce ujemanja v kodni tabeli ni, zapise kar znak.
	Uporablja dve kodni tabeli:
	\begin{itemize}
		\item \textbf{tabela za znake in dolzine} - 285 simbolov
		      (0-255 za osnovne znake, 256 konec bloka, 257-285 kodira dolzine)
		      Kodne zamenjave brez dodatnih bitov, se zakodira s Huffmanom.
		\item \textbf{tabela odmikov}
	\end{itemize}
	Niz znakov se razdeli na bloke(64k)
	vsak blok se kodira na enega od treh nacinov:
	\begin{enumerate}
		\item \textbf{brez stiskanja} osnovni znaki se prepisejo
		\item \textbf{stiskanje s staticnim Huffmanom} (verjetnosti podane vnaprej), Huffmanovo drevo ni zakodirano v bloku
		\item \textbf{stiskanje s Huffmanom} izracunamo verjetnosti za vsak blok
	\end{enumerate}
	Glava posameznega bloka: 1bit - zadnji/ni zadnji blok + 2bita tip stiskanja + pri (3) se Huffmanovo drevo
	Ker Huffmanovo drevo ni enolicno, uvedemo kanonicni Huffmanov kod.

	\subsubsection{Kanonicni Huffmanov kod}
	Postopek kodiranja:
	\begin{enumerate}
		\item znake razvrstimo najprej po dolzinah kodnih zamenjav in nato po abecedi(narascujoce)
		\item prvi simbol ima same nicle
		\item vsakemu naslednjemu znaku dodelimo naslednjo binarno kodo (prejsnji + 1)
		\item ce je kodna zamenjava daljsa od binarne kode stevila, na koncu pripnemo niclo
		\item ponavjlaj (3) do konca
	\end{enumerate}
	Na taksen nacin dosezemo, da je potrebno kodirati samo dolzine kodnih zamenjav.

	\subsection{Kod Lempel-Ziv (LZW)}
	Osnovni slovar je podan in ga sporti doponjujemo. Alogritem za \textbf{kodiranje}:
	\begin{verbatim}
N = ""
ponavljaj:
    preberi naslednji znak z
    ce je [N,z] v slovarju:
        N = [N, z]
    drugace:
        izpisi indeks k niza N
        dodaj [N, z] v slovar
        N = z
izpisi indeks k niza N
\end{verbatim}
	Algoritem za \textbf{dekodiranje}:
	\begin{verbatim}
preberi indeks k
poisci niz N, ki ustreza indeksu k
izpisi N
L = N
ponavljaj:
    preberi indeks k
    ce je k v slovarju:
        poisci niz N
    drugace:
        N = [L, L(1)]
    izpisi N
    v slovar dodaj [L, N(1)]
    L = N
\end{verbatim}
	LZW doseze optimalno stiskanje, pribliza se entropiji. % dodaj dokaz ce bo potrebno

	\subsection{Verizno kodiranje ali RLE (run lenght encoding)}
	Namesto originalnih podatkov, sharnjujemo dolzino verige (fffeef $\rightarrow$ 3f2e1f).
	Problemu, ko se podatki ne ponavljajo, se izognemo tako, da izvedemo kombinacijo direktnega kodiranja
	in kodiranja RLE. % dodaj podrobnosti ce bo potrebno


	\subsection{Stiskanje z izgubami}
	S taksnim nacinom stiskanja lahko dosezemo veliko boljsa kompresijska razmerja, vendar izgubimo podatke.
	Zato ga uporabljamo samo s formati, kjer se ne ukvarjamo z integriteto podatkov(MP3, MPEG, JPEG, $\dots$).
	Postopki kodiranj znanih formatov:
	\begin{itemize}
		\item \textbf{JPEG}
		      \begin{enumerate}
			      \item priprava slike $\rightarrow$ ker je svetlost bol pomembna, je barvna resolucija obicajno zmanjsana ($YC_RC_B$)
			      \item aproksimacija vsake od treh komponent s 2D DCT
			      \item kvantizacija $\rightarrow$  podatki ki bolj izstopajo so shranjeni manj natancno kot tisti ki so staticni
			      \item kodiranje blokov s pomocjo entropije
			      \item RLE cik-cak po sliki
			      \item RLE kodiramo z Huffmanom ali Aritmenticnim kodom
		      \end{enumerate}
		\item \textbf{MP3}
		      \begin{enumerate}
			      \item Modified DCT
			      \item odstranitev za cloveka neslisnih frekvenc
			      \item stereo, ce sta si L in R pretvorimo v mono
			      \item Huffman na koncu
		      \end{enumerate}
		\item \textbf{MPEG}
		      \begin{enumerate}
			      \item uvodno kodiranje $\rightarrow$ celotna slika JPEG
			      \item nato pa kodiramo samo spremembe, ki so se zgodile v sliki JPEG s pomocjo vektorja premika. V
			            primeru, da je prevec razlik, se ponovno kodira JPEG slika.
		      \end{enumerate}
	\end{itemize}

	\subsection{Kompresijsko razmerje}
	Izracunamo ga po formuli $\rightarrow$ stisnjeni binarni zapis C(M) / binarni zapis dokumenta (M):
	\begin{center}
		\begin{math}
			R = C(M) / M
		\end{math}
	\end{center}

	\section{Kanali}

	\subsection{Uvod}
	Kanali so strukture, ki opisujejo medsebojno povezanost. Kanal prenasa informacijo o spemenljivki
	$X$ do spremenljivke $Y$. Matematicno ga opisemo s \textbf{pogojnimi verjetnostmi}, ki povezujejo izhodne
	verjetnosti z vhodom.

	\subsection{Diskretni kanal brez spomina}
	Povezuje diskretni nakljuicni spremenljivki, s koncno mnozico stanj $X=\{x_1, \dots, x_r\}$ in $Y=\{y_1, \dots, y_s\}$.
	Obema nakljicnima spremenljivkama pripadajo tudi posamezne verjetnosti $P_X = \{p(x_1), \dots, p(x_r)\}$ in $P_Y = \{p(y_1), \dots, p(y_s)\}$. Kjer velja, da je vsota posamezne mnozice verjetnosti enaka 1. Kanal je definiran kot mnozica \textbf{pogojnih verjetnosti}
	\begin{center}
		$p(y_j | x_i)$.
	\end{center}
	Pogojna verjetnost nam pove verjetnost za dogodek $y_j$ na izhodu iz kanala, ce je na vhodu v kanal dogodek $x_i$.
	Brez spomina je zato, ker so pogojne vrjetnosti konstantne in torej neodvisne od prehodnih simbolov, velja
	\begin{center}
		\begin{math}
			\sum_j p(y_j | x_i) = 1.
		\end{math}
	\end{center}
	Kanal popolnoma podamo z $r \times s$ pogojnimi verjetnostmi.

	\subsubsection{Binarni simetricni kanal (BSK)}
	Gre za poseben primer diskretnega kanala brez spomina. Napaka kanala je $p$, saj se z verjetnostjo $p$ znak prenese
	v napacnega.
	\begin{center}
		\begin{math}
			P_k =
			\begin{pmatrix}
				1-p & p   \\
				p   & 1-p
			\end{pmatrix}
		\end{math}
	\end{center}

	\subsection{Pogojna entropija}
	Pogojna entropija spremenljivke $Y$ pri znanem $X$ se zapise kot $H(Y|X)$.
	Vzemimo, da se je zgodil dogodek $x_i \in X$. Entropija dogodka $Y$ je potem
	\begin{center}
		\begin{math}
			H(Y|x_i) = - \sum_{j=1}^s p(y_j|x_i) \log(p(y_j| x_i)).
		\end{math}
	\end{center}
	Velja: $0 \leq H(Y| x_i)$.\\
	Ce pa o dogodku X vemo le da se je zgodil, se lahko spomnemo na vis in uporabimo
	\textbf{vezano verjetnost} dogodkov $X$ in $Y$, ki pravi:
	\begin{center}
		\begin{math}
			p(x_i, y_j) = p(y_j|x_i)p(x_i)
		\end{math}
	\end{center}
	Za entropijo:
	\begin{center}
		\begin{math}
			H(Y|X) = \sum_{i} p(x_i)H(Y|x_i)
		\end{math}
		\begin{math}
			= -\sum_{i=1}^r \sum_{j=1}^s p(x_i, y_i) \log p(y_j | x_i)
		\end{math}
	\end{center}
	Splosno velja: $0 \leq H(Y|X) \leq H(Y)$, ce poznamo spremenljivko $X$, se nedolocenost $Y$ ne more povecati (lahko se pomanjsa).

	\subsubsection{Pogojna verjetnost}
	Verjetnost da se zgodi dogodek A, ce vemo, da se zgodi dogodek B, je
	\begin{center}
		\begin{math}
			P(A | B) = \dfrac{P(A \cap B)}{P(B)} = \dfrac{P(A)P(B|A)}{P(B)}
		\end{math}
	\end{center}
	Dogodka $A$ in b sta \textbf{neodvisna}, ce velja $P(A | B) = P(A)$ ali
	$P(A B) = P(A)P(B)$.
	Pazi! Za par \textbf{nezdruzljivih} dogodkov $A$ in $B$
	pa velja $P(AB) = 0$,  $P(A + B) = P(A) + P(B)$, $P(A|B) = 0$ in $P(B|A) = 0$.

	\subsubsection{Popolna verjetnost}
	Dogodki $H_{1}, H_{2}, \dots H_{n}$ tvorijo \textbf{popoln sistem dogodkov},
	ce se nobena dva dogodka ne moreta zgoditi hrkati in se vedno
	zgodi vsaj en od njih. Ce dogodki izpolnjujejo ta pogoj, potem po
	nacelu vkljucitev/izkljucitev velja:
	\begin{center}
		\begin{math}
			P(A) =
		\end{math}
		\smallskip
		\begin{math}
			\sum_{i=1}^{\infty} P(A \cap H_{i}) =
			\sum_{i=1}^{\infty} P(H_{1}) P(A | H_{i})
		\end{math}
	\end{center}

	\subsection{Vezana entropija spremeljivk}
	Vezana entropija nakljucnih spremenljivk $X$ in $Y$ je entropija para $(X, Y)$.
	Pomembne zveze:
	\begin{itemize}
		\item $p(x_i, y_j) = p(y_j| x_i)p(x_i)$,
		\item $\sum_j p(x_i, y_j) = p(x_i)$,
		\item $\sum_i p(x_i, y_j) = p(y_j)$,
		\item $\sum_{i,j} p(x_i, y_j) = 1$ (pazi pri racunskih!)
	\end{itemize}
	Velja:

	\begin{center}
		$H(X, Y) = H(Y|X) + H(X)$
	\end{center}
	kar nam pove, da ce najprej izvemo, kaj se je zgodilo v dogodku $X$ in potem
	dobimo se dodatne informacije od dogodku $Y$, vemo vse.
	\subsubsection{Obrat kanala}
	Ker velja tudi $H(X, Y) = H(X|Y) + H(Y)$, kanal lahko \textbf{obrnemo}
	(sepravi vhod $Y$ in izhod $X$). Pri tem ne obracamo fizicnega procesa, ampak samo verjetnostno strukturo, ki definira kanal. \textbf{Pogoj:}  poznati moramo vhodne verjetnosti. Iz njih lahko dolocimo izhodne verjetnosti, ki jih lahko
	uporabimo kot vhodne verjetnosti v obrnjeni kanal.
	Lastnosti:
	\begin{itemize}
		\item izracun izhodnih verjetnosti $p(y_j) = \sum_i p(y_j, x_i)p(x_i)$
		\item obratne pogojne vrjetnosti $p(x_i, y_j)= p(y_j|x_i)p(x_i) = p(x_i|y_j)p(y_j)$
	\end{itemize}
	Za sprejemnika sporocila so obratne pogojne verjetnosti zelo pomembne, saj z njimi lahko iz prejetih znakov doloci
	verjetnost za vhodne znake.

	\subsection{Medesebojna informacija}
	Pove nam, koliko o eni spremenljivki izvemo iz druge spremenljivke,
	definicija:
	\begin{center}
		\begin{math}
			I(X;Y) = H(X) - H(X|Y)
		\end{math}
	\end{center}
	Lastnosti:
	\begin{itemize}
		\item $I(X;Y) = H(X, Y) - H(X|Y) - H(Y|X)$
		\item $I(X;Y) = H(X) - H(X|Y)$
		\item $I(X;Y) = H(Y) - H(Y|X)$
		\item $I(X;Y) = H(X) + H(Y) - H(X, Y)$
		\item $I(X;Y) = \text{simetricna glede na } X \text{ in } Y$
		\item $I(X;Y) = -\sum_i\sum_j p(x_i, y_j) \log \frac{p(x_i)p(y_j)}{p(x_i, y_j)}$
		\item $I(X;Y) \geq 0$
		\item $I(X;X) = H(X)$
	\end{itemize}
	Iz definicije medsebojne informacije lahko sklepamo:
	\begin{center}
		$0 \leq H(X|Y) \leq H(X)$.
	\end{center}

	\subsection{Kapaciteta kanala}
	Kapaciteta kanala je najvecja mozna medsebojna informacija, ki jo lahko prenesemo od vhoda na izhod.
	\begin{center}
		$C =\underset{P(X)}{\max} I(X;Y)$
	\end{center}
	\subsubsection{Kapaciteta kanala BSK}
	Lastnosti:
	\begin{itemize}
		\item $C =\underset{P(X)}{\max} (H(Y) - H(Y|X))$
		\item $p(x_0) = \alpha, p(x_1) = 1 - \alpha$
		\item $I(X;Y) = H(Y) - H(Y|X) = \dots = H(Y) - H(p, 1-p)$
		\item $\frac{dI(X;Y)}{d \alpha} = 0$
		\item $H(Y) = 1 \Rightarrow C$ je max
		\item $C=I(X;Y) |_{\alpha = 1/2} = 1 - H(p, 1-p)$
	\end{itemize}
	\subsubsection{Kapacitata kanala BSK z brisanjem}
	Definicija:
	\begin{center}
		\begin{math}
			P_k =
			\begin{pmatrix}
				1-p & p & 0   \\
				0   & p & 1-p
			\end{pmatrix}
		\end{math}
	\end{center}
	Lastnosti:
	\begin{itemize}
		\item $C = 1 - p$
		\item $p(x_0) = \alpha, p(x_1) = 1 - \alpha$
		\item $p(y_0) = (1-p)\alpha, p(y_1) = p, p(y_2) = (1-p)(1-\alpha)$
		\item $I(X;Y) = (1-p)H(\alpha, 1 - \alpha)$
		\item $\frac{dI(X;Y)}{d \alpha} = 0 \Rightarrow \alpha = 1/2$
	\end{itemize}

	\subsection{Shannonov drugi teorem}
	Shannon je ugotovil, da nam zdruzevanje znakov v nize daje vec moznosti za doseganje zaneslijvega prenosa.\\
	Naj bo $M$ stevilo razlicnih kodnih zamenjav, ki jih lahko oblikujemo z nizi dolzine $n$. Potem je \textbf{hitrost koda}
	(prenosa)  definirana kot:
	\begin{center}
		\begin{math}
			R = \frac{max H(X^n)}{n} = \frac{log M}{n} = \frac{k}{n}
		\end{math}
	\end{center}
	Hitrost je najvecja takrat, ko so dovoljene kodne zamenjave na vhodu enako verjetne. Shannonov teorem pravi, da je mozna
	skoraj popolna komunikacija s hitrostjo, enako kapaciteti kanala.
	\textbf{Teorem:}\\
	\begin{center}
		Za \textbf{R $\leq$ C} obstaja kod, ki zagotavlja tako preverjanje informacije, da je verjetnost napake pri
		dekodiran  poljubno majhna. Za \textbf{R $>$ C} kod, ki bi omogocal preverjanje informacije s poljubno majhno
		verjetnostjo napake, \textbf{ne} obstaja.
	\end{center}
	Ce so znaki neodvisni, velja:
	\begin{center}
		\begin{math}
			\log(H(X^n)) = n \log H(X) \Rightarrow R = H
		\end{math}
	\end{center}
	Za $R \leq \frac{\log 2^{nC}}{n} = C$ je mozno najti kodne zamenjave, ki omogocajo zanesljivo komunikacijo.

	\section{Varno kodiranje}

	\subsection{Uvod}
	Omejili se bomo na enostavne linearne blocne kode za BSK. Dolzina bloka je $k$ znakov, abeceda je enaka abecedi
	kanala, torej imamo $M=2^k$ blokov $x_1, \dots x_k$, $x_i \in \{0, 1\}$. Za potrebe varovanja dodamo se nekaj
	varnostnih znakov, celotna dolzina vsake od M kodnih zamenjav je potem n. Namesto enega posljemo n enakih znakov.
	Boljsi pristop pa je, da naredimo kode, kjer se povecujeta $n$ in $k$ hitreje od razilke $n -k$.

	\subsection{Kontrolne vsote}
	Varnost komunikacije povecamo tako, da dodamo nekaj bitov za preverjanje parnosti(paritetni biti).
	Nastavljeni so tako, da je vsota bitov v aritmetiki po modulu 2 fiksna vrednost (0 ali 1).\\
	\begin{center}
		\begin{tabular}{ |c|c|c| }
			\hline
			+/-/XOR & 0 & 1 \\
			\hline
			0       & 0 & 1 \\
			1       & 1 & 0 \\
			\hline
		\end{tabular}
	\end{center}

	npr. $00|0, 01|1, 10|1, 11|1$(detektiramo samo eno napako).

	\subsubsection{Pravokotni kodi}
	Zapisemo ga v obliki pravokotnika, gledamo sodost po vrsticah in po stolpcih.

	\begin{center}
		\begin{tabular}{ |c|cc| }
			\hline
			1 & 0 & 1 \\
			0 & 1 & 1 \\
			\hline
			0 & 1 & 0 \\
			\hline
		\end{tabular}
	\end{center}

	\subsubsection{Trikotni kodi}
	Vsota elementov v stolpcu in vrstici s paritetnim bitom vred mora biti soda. (ravno tako vsota paritetnih bitov)
	\begin{center}
		\begin{tabular}{ ccc }
			1          & 0          & \textbf{1} \\
			0          & \textbf{0} &            \\
			\textbf{1} &            &            \\
		\end{tabular}
	\end{center}

	\subsection{Hammingova razdalja}
	Hammingova razdalja med kodnima zamenjava nam pove stevilo znakov, na katerih se razlikujeta. Kodni zamenjavi sta enaki,
	ce je razdalja 0, razdalja med razlicnimi kodi mora biti vsaj 1, drugace je kod \textbf{singularen}.
	Razdalja je podana kot \textbf{minimalna} Hammingova razdalja med dvema kodnima zamenjavama.
	Stevilo napak, ki jih kod zazna:
	\begin{center}
		$d_H \geq e + 1 \rightarrow e_{max} = d_H-1$
	\end{center}
	\begin{center}
		$d_H \geq 2f + 1 \rightarrow f_{max} = \lfloor \frac{e_{max}}{2} \rfloor = \lfloor \frac{d_H-1}{2} \rfloor$
	\end{center}

	\subsubsection{Hammingov pogoj}
	Za bloke dolzine $n$ lahko zgradimo $2^n$ razlicnih kodnih zamenjav. Ce zelimo zagotoviti odpornost na napake,
	mora biti razdalja $d > 1$. Uporabni kodi imajo st. kodnih zamenjav $M = 2^k < 2^n$.
	Hammingov pogoj: da bi lahko dekodirali vse kodne zamenjave, pri katerih je prislo do $e$ ali manj napak
	mora veljati:
	\begin{center}
		\begin{math}
			M \leq \dfrac{2^n}{\sum_{i=0}^e{n \choose i}}
		\end{math}
	\end{center}
	\textit{Spomnimo se:}
	\begin{math}
		{n \choose i} = \frac{n!} {i! (n - i)!}, n! = n(n -1)!
	\end{math}

	\subsection{Linearni blocni kodi}
	Kode oznacimo kot dvojcek $(n, k)$. $n$ predstavlja stevilo vseh bitov, $k$ podatkovnih, $n - k$ pa st. paritetnih.
	O linearnih blocnih kodih govorimo, kadar:
	\begin{itemize}
		\item je vsota vsakega para kodnih zamenjav spet kodna zamenjava.
		\item da produkt kodne zamenjave z 1 in 0 spet kodno zamenjavo.
		\item vedno obstaja kodna zamenjava s samimi niclami
	\end{itemize}
	Oznacimo jih z $L(n, k)$. \textbf{Hammingova razdalja} linearnega koda je enaka stevilu enic v kodni zamenjavi
	z najmanj enicami.
	\begin{center}
		\begin{math}
			d_H = \min d(\vec{a}, \vec{b}) = \min d(0, \vec{b} - \vec{a}) = \min d(0 , \vec{b} + \vec{a})
		\end{math}
	\end{center}

	Naj bodo podatkovni biti oznaceni kot $z_1, z_2 \text{ in } z_3$, varnostni pa kot $s_1, s_2 \text{ in } s_3$:
	\begin{center}
		\begin{tabular}{ ccc }
			$z_1$ & $z_2$ & $s_3$ \\
			$z_3$ & $s_2$ &       \\
			$s_1$ &       &       \\
		\end{tabular}
	\end{center}
	Potem vrednosti zlozimo v vektor, in opravimo kodno zamenjavo.
	\begin{center}
		\begin{math}
			\vec{x} = (x_1, x_2, x_3, x_4, x_5, x_6) = (z_1, z_2, z_3, s_1, s_2, s_3)
		\end{math}
	\end{center}
	Velja:
	\begin{center}
		$z_1 + z_2 + s_1 = 0 = x_1 + x_2 + x_4$\\
		$z_3 + s_2 + z_2 = 0 = x_2 + x_3 + x_5$\\
		$s_3 + s_3 + z_1 = 0 = x_1 + x_3 + x_6$\\
	\end{center}


	\subsubsection{Generatorska matrika}
	Generiranje kodne zamenjave lahko opisemo z generatorsko matriko.

	\begin{center}
		\begin{math}
			\vec{x} = \vec{z}G =
			\begin{bmatrix}
				1 & 0 & 0 & 1 & 0 & 1 \\
				0 & 1 & 0 & 1 & 1 & 0 \\
				0 & 0 & 1 & 0 & 1 & 1 \\
			\end{bmatrix}
		\end{math}
	\end{center}

	V splosnem podatkovni vektor $1 \times k$ mnozimo z generatorsko matriko $k \times n$, da dobimo kodno zamenjavo
	$1 \times n$. Matrika mora imeti linearno neodvisne vrstice. Kod, cigar generatorska matrika ima to obliko, je
	\textbf{sistematicni kod} - prvih $k$ znakov koda je enakih sporocilu (podatkovnim bitom), ostalih $n-k$ znakov pa so
	paritetni biti.

	Za diskretne kanale brez spomina (sistematicne kode) jo vedno lahko zapisemo v obliki $G = (I_k | A)$.

	\subsubsection{Matrika za preverjanje sodosti}
	Linearne enacbe lahko zapisemo z matriko za preverjanje sodosti:

	\begin{center}
		\begin{math}
			H =
			\begin{bmatrix}
				1 & 1 & 0 & 1 & 0 & 0 \\
				0 & 1 & 1 & 0 & 1 & 0 \\
				1 & 0 & 1 & 0 & 0 & 1
			\end{bmatrix}
		\end{math}
	\end{center}
	Lastnosti sist. kodov:
	\begin{itemize}
		\item $\vec{x}H^T = 0$
		\item $G = (I_k | A) \Rightarrow H = (A^T | I_{n-k})$
		\item $GH^T = 0$
		      \begin{center}
			      \begin{math}
				      GH^T = (I_k | A) {A^{T^{T}} \choose I_m} = (I_k | A) {A \choose I_m} = I_k A + A I_m = AA = \vec{0}
			      \end{math}
		      \end{center}
		\item vsota dveh kodnih zamenjav je nova kodna zamenjava.
	\end{itemize}

	\subsection{Sindrom v kanalu}
	Predpostavimo da se med posiljanjem v kanalu zgodi napaka:
	\begin{center}
		\begin{math}
			z \rightarrow x = zG \rightarrow err\rightarrow y = x + e \rightarrow s = yH^T
		\end{math}
	\end{center}
	Napako pri prenosu preprosto ugotavljamo tako, da pogledamo, ce je $s = 0$. Vendar to nam ne garantira da pri prenosu ni prislo do napake.
	Sindrom izracunamo na naslednji nacin(vektor velikosti $1 \times n - k$):
	\begin{center}
		\begin{math}
			yH^T = (x + e)H^T = 0 + eH^T = s
		\end{math}
	\end{center}
	Ker je verjetnost za napako obicajno $p << 1$, je niz s $t$ napakami veliko
	verjetnejsi od niza s t + 1 napakami.

	\subsubsection{Standardna tabela}
	Imejmo ponavljalni kod $(0|00)$ in $(1|11)$. Sestavimo matrki G in H.

	\begin{math}
		G = [1|11] \text{ in } H =
		\begin{bmatrix}
			1 | & 1 & 0 \\
			1 | & 0 & 1
		\end{bmatrix}
	\end{math}


	Imamo 4 mozne sindrome: (00), (01), (10), (11). Na izhodu lahko dobimo $2^n = 8$
	razlicnih nizov.

	Mozne nize na izhodu in njihove sindrome obicajno razvrstimo v std. tabelo:
	\begin{center}
		\begin{tabular}{ c|cc }
			\text{sindrom} & popravljalnik &     \\
			\hline
			00             & 000           & 111 \\
			01             & 001           & 110 \\
			10             & 010           & 101 \\
			11             & 100           & 011
		\end{tabular}
	\end{center}

	V isti vrstici so nizi, ki dajo enak sindrom. V prvi vrstici so vedno kodne zamenjave, ki
	imajo sindrom 0. Skrajno levo je vedno niz, ki ima najmanj enic, saj je najbolj verjeten.
	Imenujemo ga popravljalnik. Ostale nize dobimo tako, da popravljalnik pristevamo k kodnim
	zamenjavam v prvi vrsti. Popravljanje je sedaj enostavno: izracunamo sindrom, popravljalnik
	odstejemo(pristejemo) od prejetega niza.


	\subsection{Hammingov kod}
	Hammingovi kodi so druzina linearnih blocnih kodov, ki lahko popravijo eno napako.
	Najlazje jih predstavimo z matriko za preverjanje sodosti, v kateri so vsi stolpci
	nenicelni vektorji. Spadajo med \textbf{popolne kode} - sfere z radijem 1 okrog kodnih
	zamenjav ravno napolnijo prostor z $2^n$ tockami.

	Kod z $m$ varnostnimi biti ima kodne zamenjave dolzine $2^m - 1$.  Oznaka koda je potem
	$H(2^m - 1, 2^m - 1 - m)$. Ce stolpce v matriki H interpretiramo kot stevila v binarni
	obliki, nam oznaka stolpca doloca polozaj napake. Stolpci v Hammingovem kodu so lahko
	poljubno razmetani. Pomembno je le to, da nastopajo \textbf{vsa} stevila od 1 do $2^m - 1$.

	Hammingov kod je lahko:
	\begin{itemize}
		\item \textbf{leksikografski} - oznake stolpcev si sledijo po vrsti

		\item \textbf{sistematicni} - oznake stolpcev so pomesane

		      \textit{Iz pozicije varnostnih bitov lahko pridobimo enacbe, s pomocjo katerih
			      lahko potem sestavimo generatorsko matriko. Sepravi vsota vseh $z_i$, kjer se nahaja $s_i$}
	\end{itemize}

	V Hammingovem kodu se za varnostne bite obicajno vzamejo tisti stolpci, ki imajo samo \textbf{eno} enico.

	\subsubsection{Dekodiranje}
	Dekodiranje leksikografskega Hammingovega koda je preprosto:
	\begin{enumerate}
		\item izracunamo sindrom $s = yH^T$
		\item ce je $s = 0$, je $x' = y$
		\item ce $s \neq 0$, decimalno stevilo $S$ predstavlja mesto napake.
	\end{enumerate}
	Za kod, ki pa ni leksikografski potrebujemo tabelo povezav med indeksi sindromov in stolpci(sepravi pogledamo, na kateri indeks se slika izracunani sindrom).

	\subsection{Ciklicni kodi}
	Ciklicni kod $C(n, k)$ je linearni blocni kod, v katerem vsak krozni premik kodne zamenjave
	da drugo kodno zamenjavo. Zapisemo jih s polinomi po padajocih potencah (ravno tako jih sestevamo po mod 2).

	\subsubsection{Zapis s polinomi}
	Imejmo osnovni vektor:
	\begin{center}
		$x = (x_{n-1}, x_{n-2}, \dots, x_0) \Leftrightarrow$\\
		$x(p) = x_{n-1}p^{n-1} + x_{n-2}p^{n-2} + \dots + x_0$
	\end{center}
	Izvedemo premik za eno mesto:
	\begin{center}
		$x' = (x_{n-2}, \dots, x_0, x_{n-1}) \Leftrightarrow$\\
		$x'(p) = x_{n-2}p^{n-2} + \dots + x_0p + x_{n-1}$
	\end{center}
	Velja zveza: $x'(p) = px(p) - x_{n-1}(p^n -1)$.

	V mod $2$ aritmetiki:
	\begin{center}
		$\Rightarrow$ $x'(p) = px(p) + x_{n-1}(p^n -1)$.
	\end{center}

	V $\text{mod}(p^n + 1)$ aritmetiki:
	\begin{center}
		$\Rightarrow$ $x'(p) = px(p) \text{ mod}(p^n + 1)$.
	\end{center}

	\textbf{Pozor:} aritmetiko po mod 2 izvajamo na \textbf{istih} stopnjah polinoma (na bitih), aritmetiko
	po mod $(p^n + 1)$ pa na \textbf{polinomu}.

	Izvajanje kroznega prekmika za $i$ mest:
	\begin{center}
		\begin{math}
			x^i(p) = p^i x(p) \text{ mod} (p^n + 1)
		\end{math}
	\end{center}

	\subsubsection{Generatorski polinomi}
	Vrstice generatorske matrike lahko razumemo kot kodne zamenjave.
	Za ciklicne kode v splosnem velja: \textbf{Generatorski polinom} je stopnje $m$, kjer je $m$ stevilo
	varnostnih bitov, in ga oznacimo kot:
	\begin{center}
		\begin{math}
			g(p) = p^m + g_{m-1}p^{m-1} + \dots + g_1p + 1
		\end{math}
	\end{center}
	Za sistematicni kod velja: $G = [I_k | A_{k, n-k}]$.
	Generatorska matrika:
	\begin{center}
		$G= $
		\begin{math}
			\begin{bmatrix}
				1      & g_{m - 1} & \dots   & g_1   & 1   & 0       & \dots & 0     & 0      \\
				0      & 1         & g_{m-1} & \dots & g_1 & 1       & 0     & \dots & 0      \\
				\vdots &           &         &       &     &         &       &       & \vdots \\
				0      & 0         & \dots   & 0     & 1   & g_{m-1} & \dots & g_1   & 1
			\end{bmatrix}
		\end{math}
	\end{center}
	Sistematicni lahko dobimo z linearnimi operacijami nad vrsticami.
	Velja:
	\begin{center}
		\begin{math}
			p^n + 1 = g(p)h(p)
		\end{math}
	\end{center}
	Sepravi vsak polinom, ki polinom $p^n + 1$ deli brez ostanka, je generatorski polinom.

	\subsubsection{Polinom za preverjanje sodosti}
	Velja: $x(p)h(p) \mod(p^n + 1) = 0$ $\Rightarrow \sum_{i=0}^{n-i} x_i h_{j-i} = 0$

	V matricni obliki: $\vec{x} H^T = H\vec{x}^T = 0$
	\begin{center}
		\begin{math}
			\begin{bmatrix}
				h_0    & \dots & h_k   & 0   & \dots & 0     & 0      \\
				0      & h_0   & \dots & h_k & 0     & \dots & 0      \\
				\vdots &       &       &     &       &       & \vdots \\
				0      & 0     & \dots & 0   & h_0   & \dots & h_k
			\end{bmatrix}
			\begin{bmatrix}
				x_{n-1} \\
				\vdots  \\
				x_0
			\end{bmatrix} = 0
		\end{math}
	\end{center}

	\subsubsection{Kodiranje z mnozenjem}
	Kodne zamenjave so veckratniki generatorskega polinoma.
	Velja:
	\begin{center}
		\begin{math}
			x(p) = z(p) g(p) mod (p^n + 1)
		\end{math}
	\end{center}, kjer je z(p) polinom, ki ustreza podatkovnemu vektorju $\vec{z}$.
	Kod, ki smo ga dobili z mnozenjem, ustreza generatorski matriki, ki ima v vrsticah
	koeficiente $p^{k-1}g(p), \dots, pg(p), g(p)$, zato ni sistematicen.

	\subsubsection{kodiranje z deljenjem}
	Kodiranje na osnovi deljenja ustvari sistematicen ciklicen kod. Kodna zamenjava je zato sestavljena iz sporocila
	(podatkovnega bloka) in varnostnega bloka znakov, $x = (z | r)$.
	Polinom podatkovnega bloka je:
	\begin{center}
		\begin{math}
			z(p) = z_{k-1}p^{n-1} + \dots + z_1p^{1} + z_0p^0
		\end{math}
	\end{center}
	Ce pa polinom pomnozimo s $p^m$, dobimo na desni $m$ nicel.
	\begin{center}
		\begin{math}
			p^m z(p) = z_{k-1}p^{k-1} + \dots + z_1p^{m+1} + z_0p^m
		\end{math}
	\end{center}
	To ustreza bloku $z$, premaknjenem za $m$ znakov v levo, ($z_{k-1}, \dots, z_0, 0, \dots, 0$).

	V splosnem nastavek seveda ne bo deljiv, velja pa:
	\begin{center}
		$p^mz(p) = g(p)t(p) + r(p)$
	\end{center}
	kjer je $t(p)$ kolicnik
	$r(p)$ pa ostanek, s stopnjo manj od $m$.

	Ostanek lahko zapisemo v obliki niza, kot $(0, \dots, 0, r_{m-1}, \dots, r_0)$.

	Polinom $p^m z(p) + r(p) = g(p)t(p)$ je deljiv z $g(p)$ in je zato ustrazna kodna zamenjava.
	Kodno zamenjavo tako dobimo, ce ostanek deljenja z generatorskim polinomom pristejemo k osnovnemu
	nastavku, $(z_{k-1}, \dots, z_0 | r_{m-1}, \dots r_0)$.

	\subsubsection{Strojna izvedba kodirnika}
	Uporabljeni so trije tipi elementov: pomnilna celica tipa $D$, sestevalnik (XOR), mnozenje s konstanto (1 $|$ 0).
	Poznamo kodrianje na osnovi deljenja in na osnovi mnozenja. (insert pics here).
	Pri kodiranju se sepravi najprej na izhod posiljajo kar vhodni znaki, potem v naslednjih korakih se vsebina pomnilnih celic od zadaj naprej.

	\subsubsection{Dekodiranje}
	Dekodiranje ciklicnih kodov sloni na linearnih blocnih kodih. Vzemimo, da je pri prenosu prislo do napake $y = x + e$, ali pa zapisano v polinomski
	obliki $y(p) = x(p) + e(p) = z(p)g(p) + e(p)$.
	\begin{itemize}
		\item Najprej izracunamo sindrom. Ekvivalent enacbe $s = yH^T$ v polinomskem zapisu je $y(p) = q(p)*g(p) + s(p)$, oz. $s(p) = y(p) \text{ mod } g(p)$.
		\item Ce je ostanek deljenja $y(p)$ z $g(p)$ razlicen od nic, je prislo do napake.
	\end{itemize}
	Iz $s(p) = y(p) \text{ mod } g(p)$ sledi, da je v primeru, ko je napaka na zadnjih $m$ mestih, stopnja $e(p)$ manj kot $m$ in velja kar $e(p) = s(p)$.
	Za ostale napake pa lahko izkoristimo ciklicnost kodov:
	\begin{itemize}
		\item Naredimo trik, osnovno enacbo premaknemo za $i$ mest:
		      \begin{center}
			      $p^iy(p) = p^ix(p) + p^i e(p)$
		      \end{center}
		\item Ce najdemo pravi $i$, bo veljalo $p^i e(p) = s(p)$
		\item Pravi $i$ je tisti, pri katerem bo $e(p)$ imel najmanj enic
	\end{itemize}

	\subsubsection{Klasifikacija napak}
	Napaki, ki se pojavi na izhodu odposlane kodne zamenjave neodvisno od morebitnih napak
	na sosednjih znakih, pravimo \textbf{posamicna} ali \textbf{neodvisna} napaka. Do posamicnih
	napak pride zaradi motenj, ki so krajse od casa posiljanja enega znaka.

	Povezanim napakam na vec zaporednih znakih pravimo \textbf{izbruh}. Dolzina izbruha je stevilo znakov med
	prvim in zadnjim napacno sprejetim znakom. Do izbruha pride, ce je trajanje motenj daljse od casa posiljanja enega znaka.

	Ciklicni kodi so posebej primerni za \textbf{ ugotavljanje izbruhov napak }.

	\subsubsection{Zmoznosti ciklicnih kodov}
	Odkrivanje napak s ciklicnimi kodi, kjer velja $1 < \text{ st }(g(p)) < n$:
	\begin{itemize}
		\item Kod odkrije vsako posamicno napako: $e(p) = p^i$
		      \begin{center}
			      \begin{math}
				      e(p) = p^i \text{ ni deljiv z } g(p)
			      \end{math}
		      \end{center}
		\item Za dolocene generatorske polinome odkrije tudi dve posamicni napaki do dolzine bloka $n = 2^m -1$
		      \begin{center}
			      \begin{math}
				      e(p) = p^i + p^j = p^j(p^{i-j} + 1)
			      \end{math}
		      \end{center}
		      \textit{Pri pogoju, da $p^j$ ni deljiv z $g(p)$ in $p^{i-j}$ ni deljiv z $g(p)$ za vsak $i-j$}

		\item Odkrije poljubno stevilo lihih napak, ce $p + 1$ deli $g(p)$
		      \begin{center}
			      \begin{math}
				      g(p) = (p + 1) g_g(p)
			      \end{math}\\
			      \begin{math}
				      (p + 1) p^i =  p^{i+1} + p^i
			      \end{math}\\
			      \begin{math}
				      (p + 1)(p^i + p^{i - 1}) = p^{i + 1} + p^i + p^i + p^{i-1} = p^{i + 1} + p^{i-1}
			      \end{math}\\
		      \end{center}
		\item Odkrije vsak izbruh napak do dolzine $m$
		\item Odkrije vse razen $2^{-(m-1)}$ izbruhov dozline $m + 1$
		\item Odkrije tudi vse razen delez $2^{-m}$ izbruhov daljsih od $m + 1$
	\end{itemize}
	Popravljanje napak s ciklicnimi kodi, kjer velja $1 < \text{ st }(g(p)) < n$:
	\begin{itemize}
		\item Izracun sindroma
		\item Ciklicno prilaganje sindroma prenesenemu blok $y$.
		\item Popravijo lahko do $e = \lfloor \frac{d-1}{2} \rfloor$ posamicnih napak, kjer je
		      $d$ Hammingova razdalja koda.
		\item Popravijo lahko tudi izbruhe napak do dolzine $e = \lfloor \frac{m}{2} \rfloor$
	\end{itemize}

	\subsubsection{CRC}
	Ali Cyclic Redundancy Check, temelji na coklicnih kodih.
	Po standardu velja:
	\begin{itemize}
		\item Registri v \textbf{LSFR} so na zacetku nastavljeni na \textbf{1}; osnovni CRC ne loci sporocil,
		      ki imajo razlicno stevilo vodilnih nicel. Ta sprememba, ki je ekvivalentna negiranju prvih $m$ bitov,
		      to tezavo odpravi.
		\item Na koncu sporocila dodamo $m$ - bitov, odvisno od implementacije LSFR. Pri nasi se to ne dela!
		\item \textbf{Operacija XOR} na fiksnem ostanku deljenja, obicajno je to kar negacija vseh bitov.
		\item \textbf{Vrstni red bitov v bajtu} - nekateri serijski protokoli najprej oddajo najmanj pomembne bite
		      (najmanj pomembni bit ima najvisjo stopnjo polinoma).
		\item \textbf{Vrsni red bajtov} - pomnilniska organizacija, odvisna od arhitekture (LE, BE).
		\item Notacija CRC polinomov - biti oznacujejo prisotnost faktorja. Veckrat se izpusca en izmed faktorjev $p^m \text{ ali } 1$.
	\end{itemize}
	Ciklicni kodi so odlicni za detekcijo napak. Za popravljanje napak pa danes obstajajo boljsi kodi.

	\subsubsection{Prepletanje}
	Motnje so mnogokrat v obliki izbruhov. V takih primerih pride na dolocenih kodnih zamenjavah do velikega stevila napak, na drugih pa napak ni.
	S prepletanjem bitov se da napake porazdeliti med vec kodnih zamenjav.
	Resitev:
	\begin{itemize}
		\item Kodne zamenjave v kodirnik vpisujemo vrstico po vrstico, oddaja pa jih stolpec po stolpec. Obratno je na strani dekodirnika.
		\item Naceloma je vzorec skoraj nakljucen. Matriko prepletanja poznata kodirnik in dekodirnik.
		\item Dodamo zakasnitev, izmenicno signali potujejo gor/dol, ena veja je zakasnjena.
	\end{itemize}
	Dejanske resitve so bolj kompleksne: vec vej, zakasnitve tudi do 20 vej.

	\subsubsection{Konvolucijski kodi}
	Primerni za popravljanje napak. Konvolucijske kode genriramo z linearnimi premikalnimi registri, ki so sestavljeni iz
	pomnilnih celic D in vrat XOR. Spadajo pod nelinearne kode.

	\section{Analiza signalov}
	Pri analizi signalov in sistemov je izjemno pomembna kolicina frekvenca.

	\subsection{Invariantnost sinusoid}
	Vzemimo zvezni signal, ki prehaja skozi linearni medij (sistem) kot je na primer elektricno vezje.

	V splosnem bo signal na izhodu drugacen od signala na vhody(zvok, ki ga poslusamo pod vodo je bistveno bolj popacen od tistega,
	ki ga poslusamo na zraku)

	Pomembno pri signalih pa je, da se vhodni signal v obliki sinusoide
	\begin{center}
		\begin{math}
			x(t) = A \sin (2 \pi \nu t + \theta)
		\end{math}
	\end{center}
	popaci v izhodni signal z drugacno amplitudo in fazo $\theta$, vendar ohrani frekvenco $\nu$.
	\begin{center}
		\begin{math}
			x(t) = A' \sin (2 \pi \nu t + \theta')
		\end{math}
	\end{center}
	Razlog, da se frekvenca
	ohrani je v tem, da linearne sisteme lahko zapisemo v obliki elementarnih operacij, kot so (mnozenje s konstanto, odvajanje,
	integracija, zakasnitev, vsota).

	\subsection{Fourierova transformacija}
	Vsako periodicno funkcijo ( ce je dovolj lepa ), lahko zapisemo kot kombinacijo sinusoid.
	V kombinaciji z invariantnostjo sinusoid to pomeni, da lahko:
	\begin{itemize}
		\item vsako funkcijo razstavimo na sinusoide
		\item obravnavamo obnasanje vsake sinusoide v sistemu posebej
		\item na koncu zdruzimo locene rezultate
	\end{itemize}
	Ta koncep se danes uporablja pri vsaki analizi signalov.

	\subsubsection{Fourierova vrsta}
	Funkcija je periodicna s periodo $T$, ce velja:
	\begin{center}
		\begin{math}
			x(t + T) = x(t), \forall t: -\infty < t < \infty
		\end{math}
	\end{center}
	kjer je $T$ najmanjsa pozitivna vrednost s to lastnostjo.

	Funkciji $\sin(t)$ in $\cos(t)$ sta periodicni s periodo $2\pi$  $\Rightarrow$
	Funkciji $\sin(\frac{2 \pi t}{T})$ in $cos(\frac{2 \pi t}{T})$ sta potem periodicni funkciji
	s periodo $T$ in frekvenco $\nu_0 = \frac{1}{T}$.

	Cas merimo v sekundah, frekvenco pa v stevilu ciklov na sekundo. Pri analizi
	signalov zapis veckrat poenostavimo tako, da namesto frekvence uporabimo kotno hitrost
	\begin{center}
		\begin{math}
			\omega_0 = 2 \pi \nu_0 = \frac{2\pi}{T}
		\end{math}
	\end{center}

	Visji harmoniki sinusoid s frekvenco $\nu_0$ so $\sin$ in $\cos$ funkcije s frekvencami,
	ki so veckratniki osnovne frekvence, $n \nu_0$.

	Fourier je pokazal, da lahko \textbf{vsako} periodicno funkcijo $x(t)$ s periodo $T$ zapisemo kot:
	\begin{center}
		\begin{math}
			x(t) = \frac{a_0}{2} + \sum_{n = 1}^{\infty} a_n \cos(n \omega_0 t) +
			\sum_{n = 1}^{\infty} b_n \sin(n \omega_0 t)
		\end{math}
	\end{center}
	za $n \geq 1$.

	Lastnosti preden se lotimo dokaza:
	\begin{itemize}
		\item
		      \begin{center}
			      $\int_{0}^T \cos (\omega_0 t) dt = \int_{0}^T \sin (\omega_0 t) dt = 0$
		      \end{center}
		\item
		      \begin{center}
			      $\int_{0}^T \sin (n \omega_0 t) dt = \int_{0}^T \sin (n \omega_0 t) dt = 0$ (visji harmoniki)
		      \end{center}
		\item
		      \begin{center}
			      $\sin (2 \pi \nu_1 t) \sin (2 \pi \nu_2 t) = \frac{1}{2} (\cos (2 \pi (\nu_1 - \nu_2) t) - \cos (2 \pi (\nu_1 + \nu_2) t))$
		      \end{center}
	\end{itemize}

	Se nekaj lastnosti z dokazi:
	\begin{itemize}
		\item
		      \begin{center}
			      \begin{math}
				      \cos(n \omega_0 t) cos(m \omega_0 t) = \frac{1}{2} (cos((n+m)\omega_0 t) + cos((n-m) \omega_0 t))
			      \end{math} \\
			      ce $n \neq m$ \\
			      \begin{math}
				      = \frac{1}{2} \int_0^T (cos((n+m)\omega_0 t)dt + \frac{1}{2} \int_0^T cos((n-m) \omega_0 t))dt = 0
			      \end{math} \\
			      ker velja $n + m > 0$  in $|n - m| \geq 1$ \\
			      ce pa $n = m$ \\
			      \begin{math}
				      = \frac{1}{2} \int_0^T (cos((2n)\omega_0 t)dt + \frac{1}{2} \int_0^T cos(0))dt = \frac{T}{2}
			      \end{math} \\
		      \end{center}
		\item
		      \begin{center}
			      Enako velja za produkt dveh sinusoid.
		      \end{center}
		\item
		      \begin{center}
			      \begin{math}
				      \int_0^T \sin(n \omega_0 t) \cos(n \omega_0 t) dt = 0
			      \end{math}
		      \end{center}
	\end{itemize}

	Dokaz:
	\begin{center}
		\begin{math}
			x(t) = \frac{a_0}{2} + \sum_{n = 1}^{\infty} a_n \cos(n \omega_0 t) +
			\sum_{n = 1}^{\infty} b_n \sin(n \omega_0 t)
		\end{math}
	\end{center}
	\begin{center}
		\begin{math}
			\int_0^T x(t) dt = \frac{a_0}{2} \int_0^T dt + \int \sum + \int \sum = \frac{a_0} T \rightarrow a_0 \frac{2}{T} \int_0^T x(t) dt
		\end{math}
	\end{center}
	\begin{center}
		\begin{math}
			\int_0^T x(t) \cos(n \omega_0 t) dt = a_n \frac{T}{2} \rightarrow a_n = \int_0^T x(t) \cos (n \omega_0 t) dt
		\end{math}
	\end{center}
	Enako bi lahko pokazali za $b_n$.

	To velja za vsako funkcijo, ki zadosca Dirichletovim pogojem:
	\begin{itemize}
		\item je enoznacna (za vsak $t$ ena sama vrednost)
		\item je koncna povsod, oz. njen integral je koncen
		\item je absolutno integrabilna (ima koncno energijo)
		      \begin{center}
			      $\int_0^T |x(t)| dt < \infty$
		      \end{center}
		\item mora imeti koncno stevilo ekstremov v vsakem obmocju
		\item imeti mora kncno stevilo koncnih nezveznosti v vsakem obmocju
	\end{itemize}

	Bolj kompaktna predstavitev je z uporabo \textbf{Eulerjeve formule} $e^{i \phi} = \cos(\phi) + i \sin(\phi)$, $i = \sqrt{-1}$:
	\begin{center}
		\begin{math}
			x(t) = \sum_{n = - \infty}^{\infty} c_n e^{i n \omega_0 t}
		\end{math}
	\end{center}
	Koeficienti so kompleksni:
	\begin{center}
		\begin{math}
			c_n = \frac{1}{T} \int_0^T x(t)e^{-in \omega_0} dt = \int_{\frac{-T/2}{T/2}} x(t) e^{-in \omega_0} dt
		\end{math}
	\end{center}
	Zveza med obema zapisoma:
	\begin{itemize}
		\item $n = 0: c_0 = \frac{a_0}{2}$
		\item $n > 0: c_n = \frac{a_n - i b_n}{2}$
		\item $n < 0: c_n = \frac{a_{-n} - i b_{-n}}{2}$
	\end{itemize}

	Negativne frekvence so matematicni konstrukt, ki nam pride prav pri opisovanju singalov.
	Vsako sinusoido opisemo z dvema parametroma, prej $a_n$, $b_n$, sedaj pa elegantno
	s $c_n$ in $c_{-n}$.

	\subsubsection{Fourierova transformacija}
	Fourierovo vrsto lahko posplosimo tako, da spustimo $T \rightarrow \infty$ in dobimo Fourierovo transformacijo.
	Predstavlja jedro vseh frekvencnih analiz.
	Enacba:
	\begin{center}
		\begin{math}
			x(t) = \int_{-\infty}^{\infty} X(\nu)e^{-i2 \pi \nu t} dt = \int_{-\infty}^{\infty} x(t) e^{-i \omega t} dt
		\end{math}
	\end{center}
	Manjsi kot je $T$ v casovnem prostoru, sirsi je signal v frekvencnem prostoru.

	Lastnosti Fourierove transformacije:
	\begin{itemize}
		\item linearnost: $f(t) = ax(t) + by(t) \rightarrow F(\nu) = a X(\nu) + bY(\nu)$
		\item skaliranje: $f(t) = x(at) \rightarrow F(\nu) = \frac{1}{|a|} X(\frac{1}{a} \nu)$
		\item premik: $f(t) = x(t - t_0) \rightarrow F(\nu) = e^{-i2\pi \nu t_0} X(\nu)$
		\item modulacija: $f(t) = e^{i 2 \pi t \nu_0} x(t) \rightarrow F(\nu) = X(\nu - \nu_0)$
		\item konvolucija: $f(t) = \int_{- \infty}^{\infty} x(t - \tau)y(\tau) d\tau \rightarrow F(\nu) = X(\nu) Y(\nu)$
	\end{itemize}

	\subsubsection{Diskretna Fourierova transformacija - DFT}
	Frekvenca vzorcenja $\nu_s $ (sampling) je obratno sorazmerna periodi vzorcenja $\nu_s = \frac{1}{\Delta}$.
	Postopek:
	\begin{itemize}
		\item Ocenimo Fourierovo transformacijo iz $N$ zaporednih vzorcev.
		      \begin{center}
			      \begin{math}
				      x_k = x(k \Delta), k = 0,1, \dots, N - 1
			      \end{math}
		      \end{center}
		\item Iz $N$ vzorcev na vhodu v DFT bomo lahko izracunali natanko $N$ neodvisnih tock na izhodu.
		\item Namesto, da bi dolocili DFT za vse tocke od $- \nu_C$ do $+ \nu_C$, se lahko omejimo samo
		      na dolocene vrednosti
		      \begin{center}
			      \begin{math}
				      \nu_n = \frac{n}{N \Delta}, n = - \frac{N}{2}, \dots, \frac{N}{2}
			      \end{math}
		      \end{center}
		      spodnja in zgornja meja ustrezata ravno Nyquistovi frekvenci.
		\item Trenuten zapis vkljucuje $N + 1$ vrednost. Izkazalo se bo, da sta obe robni vrednosti enaki. Imamo
		      jih zaradi lepsega zapisa.
		\item Naprej so stvari trivialne
		      \begin{center}
			      \begin{math}
				      X(\nu_n) = \int_{-\infty}^{\infty} x(t) e^{-i2 \pi \nu_n t} dt = \sum^{N-1}_{k=0} x_k e^{-i2 \pi \nu_n k \Delta} \Delta
			      \end{math}
		      \end{center}
		\item Ce v zgornji enacbi izpustimo $\Delta$, dobimo enacbo za DFT:
		      \begin{center}
			      \begin{math}
				      X_n = \sum_{k=0}^{N-1} x_k e^{\frac{-i2 \pi n k}{N}}
			      \end{math}
		      \end{center}
	\end{itemize}

	Povezava s Fourierovo transformacijo je $X(\nu_n) \approx \Delta X_n$
	Iz enacbe za DFT sledi, da je DFT periodicna s periodo $N$. To pomeni, da je $X_{-n} = X_{N-n}$
	Koeficiente $X_n$ lahko zato namesto na intervalu $[-\frac{N}{2}, \frac{N}{2}]$ racunamo na
	intervalu $[0, N - 1]$.

	Zveza med koeficienti $X_0, \dots, X_{N-1}$ in frekvencami $- \nu_C, \dots, \nu_C$:
	\begin{center}
		\begin{tabular}{ cc }
			indeks                              & frekvenca         \\
			\hline
			$n = 0$                             & $\nu = 0$         \\
			$1 \leq n \leq \frac{N}{2-1}$       & $0 < \nu < \nu_C$ \\
			$\frac{N}{2}$                       & $-\nu_C, + \nu_C$ \\
			$\frac{N}{2} + 1 \leq n \leq N - 1$ & $\nu_C < \nu < 0$ \\
			\hline
		\end{tabular}
	\end{center}

	\subsubsection{Inverzna DFT}
	\begin{center}
		\begin{math}
			x_k = \frac{1}{N} \sum_{n=0}^{N-1} X_{n}e^{\frac{i2 \pi n k}{N}}
		\end{math}
	\end{center}

	\subsection{Resonanca}
	Do resonance pride, ko je frekvenca vsiljenega nihanja enaka frekvenci lastnega nihanja. Takrat pride do ojacitve amplitud.
	Resonanca je pomembna lastnost elektricnih vezij, s katero zagotovimo nihanja, nastavljanje radijskih sprejemnikov na pravo postajo,
	odstranimo sum.

	\subsection{Modulacija in frekvencni premik}
	Iz analize vemo, da nelinearne operacije nad signali (kvadriranje, mnozenje) privedejo do
	pomembnih transformacij v frekvencnem prostoru.

	Iz osnovne trigonometrije vemo:
	\begin{center}
		\begin{math}
			\sin(2 \pi \nu_1 t) \sin(2 \pi \nu_2 t) = \frac{1}{2} [\cos(2 \pi(\nu_1 - \nu_2)t) - \cos(2 \pi (\nu_1 + \nu_2)t)]
		\end{math}\\
		\begin{math}
			\cos(2 \pi \nu t) = \sin(2 \pi \nu t + \pi / 2)
		\end{math}
	\end{center}
	Produkt sinusoid s frekvencama $\nu_1$ in $\nu_2$ lahko torej zapisemo kot vsoto sinusoide s frekvenco $\nu_1 + \nu_2$ in
	sinusoide s frekvenco $\nu_1 - \nu_2$.

	To lastnost izkorisca amplitudna modulacija (radijske postaje AM) in frekvencni premik, s katerim lahko zagotovimo
	hkraten prenos vec signalov po istem mediju.

	\subsection{Teorem vzorcenja}
	Signal moramo vzorciti vsaj s frekvenco $2 \nu_c$, ce je najvisja opazena frekvenca v signalu $\nu_c$. Na tem zakljucku
	sloni vsa danasnja tehnologija.
	\begin{center}
		$\nu_s \geq 2 \nu_c$
	\end{center}

	\subsubsection{Zajem signalov}
	Zvezni signal $x(t)$ je funkcija zvezne spremenljivke $t$. Diskreten signal je definiran samo za dolocene case, ki
	si najpogosteje sledijo v enakih casovnih intervalih $x_k = x(k \Delta)$, $\Delta$ je perioda vzorcenja.

	Signale danes obicajno zajemamo z racuanlniki. Za to se uporabljajo vezja $A/D$ pretvorniki. Imajo koncno natancnost,
	na primer 12bit. Signal torej opisemo s koncno mnogo razlicnimi amplitudami $2^{12}$.

	Diskretnemu in kvantiziranemu signalu recemo tudi digitalni signal. Kvantizacija je obicajno tako fina, da jo lahko zanemarimo.

	\subsection{Energija signala}
	Definicija:
	\begin{center}
		\begin{math}
			E = \int_{-\infty}^{\infty} x(t)^2 dt
		\end{math}
	\end{center}

	\textbf{Parsevalov teorem}
	\begin{center}
		\begin{math}
			\int_{-\infty}^{\infty} x(t)^2 dt = \int_{-\infty}^{\infty} |X(\nu)|^2 d\nu
		\end{math}
	\end{center}

	Porazdelitev energije po frekvencah podaja funckija $|X(\nu)|^2$, ki jo imenujemo \textbf{energijska spektralna gostota}.

	\subsubsection{Mocnostni spekter diskretnega kanala}
	Diskretna razlicica Parsevalovega teorema:
	\begin{center}
		\begin{math}
			\sum_{k=1}^{N-1} |x_k|^2 = \frac{1}{N} \sum_{n=0}^{N-1} |X_n|^2
		\end{math}
	\end{center}
	Pri diskretni razlicici je PSD vedno v intervalu $[- \nu_C, \nu_C]$.
	Mocnostni spekter je potem:
	\begin{itemize}
		\item $P(0) = \frac{1}{N^2} |X_0|^2$
		\item $P(\nu_n) = \frac{1}{N^2} [|X_n|^2 + |X_{N-n}|^2]$, $n = 1, 2, \dots, \frac{n}{2-1}$
		\item $P(\nu_C) = \frac{1}{N^2}|X_{\frac{N}{2}}|^2$
	\end{itemize}

\end{multicols}
\end{document}

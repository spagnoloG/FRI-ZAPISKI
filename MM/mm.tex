\documentclass{article}
\usepackage[margin=0.15cm]{geometry}
\usepackage{amsmath}
\usepackage{multicol}

\setlength{\columnseprule}{0.5pt}

\begin{document}

\begin{center}
    {\small MM/FRI \par}
\end{center}

\begin{multicols}{3}

\section{\underline{Linear Models}}

\textbf{1.1 Types of errors}\\
Errors come from: impercise data, mistakes in the model, computational percision,..
We know two types of errors:
\begin{itemize}
    \item \textbf{Absolute} error = approximate value - correct value
        \begin{center}
            \begin{math}
                \Delta x = \overline{x} - x
            \end{math}
        \end{center}
    \item \textbf{Relative} error = $\dfrac{\text{absolute err}}{\text{correct value}}$ 
        \begin{center}
            \begin{math}
                \delta x = \dfrac{\Delta x}{x}
            \end{math}
        \end{center}
\end{itemize}

\textbf{1.2} Mathematical model is \textbf{linear},
when the function F is a linear function of the parameters:
\begin{center}
    \begin{math}
        F(x, a_1, \dots, a_p) = a_1 \phi_1(x) + \dots + a_p \phi_p(x) 
    \end{math}
\end{center} 
where $\phi_1, \dots, \phi_p$ are functions of a specific type.


\textbf{1.3 Least squares method}
Given points
\begin{center}
    \begin{math}
        \{ (x_1, y_1), \dots, (x_m, y_m) \}, x_i \in R^n , y_i \in R
    \end{math}
\end{center}
the task is to find a function $F(x, a_1, \dots, a_p)$ that is good fit for the data.
The values of the parameters $a_1, \dots, a_p$ should be chosen so that the equations
\begin{center}
    \begin{math}
        y_i = F(x, a_1, \dots, a_p), i=1,\dots,m
    \end{math}
\end{center} are satisified or, it this is not possible, that the error is as small as possible.\\
We use \textbf{Least squares method} to determine that the sum od squared errors is as small as possible.
\begin{center}
    \begin{math}
        \sum_{i=1}^m (F(x_i, a_1, \dots, a_p) - y_i)^2
    \end{math}
\end{center}

\textbf{1.4 Systems of linear equations}

A system of linear equations in the matrix form is given by $A\vec{x} = \vec{b}$,
where:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item A is the matrix of coefficients of order $m \times n$ where $m$ is the number of equations
        and $n$ is the number of unknowns,
    \item $\vec{x}$ is the vector of unknowns and
    \item $\vec{b}$ is the right side vector
\end{itemize}
\begin{small}
    \begin{center}
        \begin{math}
        \begin{bmatrix}
            \phi_1(x_1) & \phi_2(x_1) & \dots  & \phi_p(x_1) \\
            \phi_1(x_2) & \phi_2(x_2) & \dots  & \phi_p(x_2) \\
            \vdots & \vdots & \ddots & \vdots \\
            \phi_1(x_n) & \phi_2(x_n) & \dots  & \phi_p(x_n)
        \end{bmatrix}
        \begin{bmatrix}
             a_1 \\
             a_2 \\
             \vdots \\
             a_p
        \end{bmatrix} = 
        \begin{bmatrix}
             y_1 \\
             y_2 \\
             \vdots \\
             y_p
        \end{bmatrix}
        \end{math}
    \end{center}
\end{small}

\textbf{1.5 Existance of solutions} in linear equations\\
Let A = $[\vec{a_1}, \dots, \vec{a_n} ]$, where $\vec{a_i}$ are vector representing
the columns of A.
For any vector $\vec{x} =$ \begin{math}
                             \begin{bmatrix}
                                  y_1 \\
                                  y_2 \\
                                  \vdots \\
                                  y_p
                             \end{bmatrix}
                           \end{math}
the produdct $A\vec{x}$ is 
a linear combination $A\vec{x} = \sum_i x_i a_i$. The system is \textbf{solvable} iff the
vector $\vec{b}$ can be expressed as a linear combination of the columns of A, that is
it is in the column space of $A$, $\vec{b} \in C(A)$.\\
By adding $\vec{b}$ to the columns of $A$ we obtain the extended matrix of the system:
\begin{center}
    \begin{math}
        [A| \vec{b}] = [\vec{a_1}, \dots, \vec{a_n} | b]
    \end{math}
\end{center}
The system $A\vec{x} = \vec{b}$ is solvable iff the rank of A equals the rank of the
extended matrix $[A| \vec{b}]$, i.e.:
\begin{center}
    \begin{math}
        rank A = rank [A| \vec{b}] =: r
    \end{math}
\end{center}
The solution is unique if the rank of the two matrices equals num of unknowns (r = n).

\textbf{1.6} Properties of \textbf{squared matrices}\\
Let $A \in R^{n \times n}$ be a square matrix. The following conditions are equivalent
and characterize when a matrix $A$ is \textbf{invertible} or \textbf{nonsingular}:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item The matrix A has an inverse
    \item rank A = n
    \item det(A) $\neq 0$
    \item The null space $N(A) = \{\vec{x}: A\vec{x} = 0$ is trivial
    \item All eigenvalues of A are nonzero
    \item For each $\vec{b}$ the system of equations $A \vec{x} = \vec{b}$ has perciesly one solution
\end{itemize}

\textbf{1.7 Generalized inverse} of a matrix
$A \in R^{n \times m}$ is a matrix $G \in R^{m \times n}$ such that
\begin{center}
    $AGA = A$
\end{center}
Let $G$ be a generalized inverse of $A$. Multiplying $AGA=A$ with $A^{-1}$ from the
left and the right side we obtain:\\
LHS: $A^{-1}GAA^{-1} = IGI = G$\\
RHS: $A^{-1}AA^{-1} = IA^{-1} = A^{-1}$\\
where I is the identity matrix. The equality LHS=RHS implies that $G=A^{-1}$.\\
Every matrix $A \in R^{n \times m}$ \textbf{has} a generalized inverse. When computing
a generalized inverse we come across two cases:
\begin{enumerate}
    \item rank $A$ = rank $A_{11}$ where
        \begin{center}
            \begin{math}
                A=
                \begin{bmatrix}
                    A_{11} & A_{12} \\
                    A_{21} & A_{22}
                \end{bmatrix}
            \end{math}
        \end{center}
        And $A_{11} \in R^{r \times r}, A_{12} \in R^{r \times (m-r)},$
        $ A_{21} \in R^{(n-r) \times r}, A_{22} \in R^{(n-r)\times (m-r)}$.
        We claim that
        \begin{center}
            \begin{math}
                G=
                \begin{bmatrix}
                    A_{11}^{-1} & 0 \\
                    0           & 0
                \end{bmatrix}
            \end{math}
        \end{center}
        where 0s denote zero matrices of appropriate sizes, is the generalized inverse of A.
    \item The upper left $r \times r$ submatrix of $A$ is \textbf{not} invertible.\\
        One way to hanlde this case is to use permutation matrices $P$ and $Q$, such that
        \begin{center}
            \begin{math}
                PAQ =
                \begin{bmatrix}
                    \tilde{A_{11}} & \tilde{A_{12}} \\
                    \tilde{A_{21}} & \tilde{A_{22}}
                \end{bmatrix}
            \end{math},
        \end{center}
        $\tilde{A_{11}} \in R^{r \times r}$ and rank $\tilde{A_{11}} = r$. By case 1 
        generalized inverse of $PAQ$ equals to
        \begin{center}
            \begin{math}
                (PAQ)^g=
                \begin{bmatrix}
                    \tilde{A_{11}^{-1}} & 0 \\
                    0           & 0
                \end{bmatrix}
            \end{math}
        \end{center}
        Thus $(PAQ) (PAQ)^g (PAQ) = PAQ$ holds,
        Multiplying from the left by $P^{-1}$ and from the right
        by $Q^{-1}$ we get:\\ 
        $A(Q (PAQ)^g P)A = A$\\
        So \begin{center}
            \begin{math}
                Q (PAQ)^g P
            \end{math}
        \end{center}
        is a \textbf{generalized} inverse of A.
\end{enumerate}
\textbf{Algorithm} for computing \textbf{$A^g$}:
\begin{enumerate}
    \item Find any nonsingular submatrix B in A of order $r \times r$,
    \item in A substitute
        \begin{itemize}
            \item elements of submatrix B for corresponding elements of $(B^{-1})^T$,
            \item all other elements with 0
        \end{itemize}
    \item the transpose of the obtained matrix is generalized inverse G
\end{enumerate}
\textbf{solutions}:\\
Let $A \in R^{n \times m}$ and $\vec{b} \in R^m$. 
If the system $A\vec{x} = \vec{b}$ is
solvable (that is, $\vec{b} \in$ C(A)) and G is a 
generalized inverse of A, then $\vec{x}=G\vec{b}$ is
a solution of the system. Moreover, 
all solutions of system are exactly vectors of the form
\begin{center}
    \begin{math}
        x_z = G\vec{b} + (GA - I)z
    \end{math}
\end{center}

\textbf{1.8 The Moore-Penrose generalized inverse}\\
The MP inverse of $A \in R^{n \times m}$ is any matrix 
$A^+ \in R^{n \times m}$ satisfying the following four conditions:
\begin{enumerate}
    \item $A^+$ is a generalized inverse of A: $AA^+A = A$
    \item A is a generalized inverse of $A^+: A^+AA^+ = A^+$
    \item The square matrix $AA^+ \in R^{n \times n}$ is symetric: $(AA^+)^T = AA^+$
    \item The square matrix $A^+A \in R^{m \times m}$ is symetric: $(A^+A)^T = A^+A$
\end{enumerate}
Properties:
\begin{itemize}
    \item If A is a square invertible matrix, then it $A^+ = A^{-1}$
    \item $((A^+))^+ = A$
    \item $(A^T)^+ = (A^+)^T$
\end{itemize}
Construction of the MP inverse (4 cases):
\begin{enumerate}
    \item $A^TA \in R^{m \times m}$ is an invertible matrix ($m \leq n$)
        \begin{center}
            \begin{math}
                A^+ = (A^TA)^{-1}A^T
            \end{math}
        \end{center}
    \item $AA^T$ is an invertible matrix ($n \leq m$)
        \begin{center}
            \begin{math}
                A^+ = A^T(AA^T)^{-1} 
            \end{math}
        \end{center}
    \item $\Sigma \in R^{n \times m}$ is diagonal matrix of the form
        \begin{center}
            \begin{math}
                \Sigma =
                \begin{bmatrix}
                    \sigma_1 &        & \\
                             & \ddots & \\
                             &        & \sigma_n
                \end{bmatrix}
            \end{math}
        \end{center}
        Then the MP inverse is:
        \begin{center}
            \begin{math}
                \Sigma =
                \begin{bmatrix}
                    \sigma_1^+ &        & \\
                               & \ddots & \\
                               &        & \sigma_n^+
                \end{bmatrix}
            \end{math}
        \end{center}
        where \begin{math}
            \sigma_i^+ =
            \Bigg \{\begin{tabular}{ccc}
                $\frac{1}{\sigma_i}$  & $\sigma_i \neq 0$ & \\
                $0$                   & $\sigma_i = 0$ & \\
            \end{tabular}
        \end{math}
    \item a general matrix A (using SVD)
        \begin{center}
            \begin{math}
                A^+ = V \Sigma^+ U^T
            \end{math}
        \end{center}
\end{enumerate}

\textbf{1.9 SVD computation}
\begin{enumerate}
    \item Compute the eigenvalues and an orthonormal basis consistiong of eigenvectors
        of the symetric matrix $A^TA$ or $AA^T$ (depending on which of them is of smaller size)
    \item the singular values of the matrix $A \in R^{n \times m}$ are equal to $\sigma_i = \sqrt{\lambda_i}$
    \item the left singular vectors are the corresponding orthonormal eigenvectors of $AA^T$
    \item the right singular vectors are the corresponding orthonormal eigenvectors of $A^TA$
    \item If u (resp. v) is a left (resp. right) singular vector corresponding to the singular
        value $\sigma_i$, then $v = Au$ (resp. $u = A^Tv$) is a right (resp left) singular
        vector corresponding to the same singular value
    \item the remaining columns of U (resp. V) consist of an orthonormal basis of the kernel
        (i.e., the eigenspace of $\lambda = 0$) of $AA^T$ (resp. $A^TA$)
\end{enumerate}

\textbf{1.10 General computation of $A^+$}
\begin{enumerate}
    \item For $A^TA$ compute its \textbf{nonzero} eigenvalues
        $\lambda_1 \geq \dots \geq \lambda_r > 0$, and the corresponding
        orthonormal eigenvectors $v_1, \dots, v_r$, and form the matrices:
        \begin{center}
            \begin{math}
                S = diag(\sqrt{\lambda_1}, \dots, \sqrt{\lambda_r}) \in R^{r \times r}
            \end{math}
            \begin{math}
                V = [v_1 \dots v_r] \in R^{m \times r}
            \end{math}
        \end{center}
    \item Put the vectors \\
         \begin{math}
             u_1 = \dfrac{Av_1}{\sigma_1}, \dots, u_r = \dfrac{Av_r}{\sigma_r}
        \end{math}\\ 
        in the matrix $U = [u_1, \dots, u_r]$
    \item Then $A^+ = V \Sigma^+ U^T$
\end{enumerate}

\textbf{1.11 Undetermined systems}\\
Let $A \in R^{n \times m}$, where $m > n$. A system of equiations that has more variables than
constraints. Typically such system has infinitely many solutions, but it may happen that it has no
solutions $\rightarrow$ such system is undetermined.
\begin{enumerate}
    \item An undetermined system of linear equations $Ax=b$ is solvable iff $AA^+b = b$.
    \item If there are infinitely many solutions, the solution $A^+b$ is the one with the
        smallest norm, i.e
        \begin{center}
            \begin{math}
                || A^+ b || = min\{||x||: Ax = b\}
            \end{math}
        \end{center}
        Moreover, it is the unique solution of smallest norm.
\end{enumerate}

\textbf{1.12 Overdetermined systems}\\
Let $A \in R^{n \times m}$, where $n > m$. A system of equations that has more constraints than
variables. Typically such system has no soluitons, but it may happen that it migh have one or
even infinitely many solutions.

\section{\underline{Nonlinear Models}}

\textbf{2.1 Definition}\\
Given is a sample of data points $\{(x_1, y_1), \dots ,(x_m, y_m)\} \in R^n \times R$.
We are searching for
\begin{center}
$F: R^n \times R^p \rightarrow R$, \\
$F(x_i, a_i, ..., a_p) = y_i,$ \\
$i = 1,...,m$
\end{center}
which can be nonlinear in parameters $a_i, \dots ,a_p$.
\textbf{Examples:} exponential decay/growth, Gaussian model, logistic model,...

We introduce a \textbf{vector function}
\begin{center}
$G: R^p \rightarrow R^m, G = (g_1,...,g_m)$\\
$g_1(a_1,..,a_p) = F(x_1,a_1,...a_p) - y_1$\\
$\vdots$\\
$g_m(a_1,..,a_p) = F(x_m,a_1,...a_p) - y_m$
\end{center}

Solving $F(x_i, a_i, ..., a_p) = y_i,$ is equivalent to solving $G(a_1,..,a_p) = (0,...,0)$. 
Newton's method will solve the latter system.

\textbf{2.2 Rate of convergence}\\
Let us say that $e_i$ is the $i$-th error of some method and $\alpha$ is
a zero we are searching for. We can show that $|e_{i+1}| \leq C \cdot \frac{1}{2}|e_i|^r$, where $C \in R>0$ and $r \in N$. $r$ is
the rate of convergence of the method.

\textbf{2.3 Case 1 for solving nonlinear equations  $(p = m = 1)$}\\
$g: R \rightarrow R$. We have to find a zero of the function $g$ (let's call it $\alpha$).

\textbf{2.3.1 Tangent method}
\begin{enumerate}
    \item Select an initial approximation $x_0$.
    \item Iterate $x_{n+1} = x_n - \dfrac{f(x_n)}{f'(x_n)}, n=0,1,...$ until it converges to $\alpha$
\end{enumerate}
Problem: the convergence of this method really needs a good initial approximation.
\textbf{Theorem:} The sequence $x_i$ from the tangent method converges with the rate
$r = 2$ if:
\begin{itemize}
  \item $f'(\alpha) \neq 0$
  \item $x_0$ was close enough to $\alpha$
\end{itemize}

\textbf{2.3.2 Bisection}
\begin{enumerate}
  \item Start with $x_0, x_1$ such that $f(x_0)f(x_1) < 0$.
  \item Compute $x_2 = \dfrac{x_0 + x_1}{2}$.
  \item Choose $[x_0, x_2]$ if $f(x_0)f(x_2) < 0$, otherwise choose $[x_2, x_1]$.
  \item Repeat.
\end{enumerate}
Bisection always converges, but it is slow.
\underline{Rate of convergence:}
We can show that $|e_{i+1}| \approx \frac{1}{2}|e_i|$. From here we can see that bisection has
a linear rate of convergence.\\

\textbf{2.3.3 Stopping criteria}\\
Iterative method: $x_0,..., x_n, x_n+1$. How do we find $n$ (how many steps of a method
do we have to make)? This has to be true:
\begin{itemize}
  \item $\dfrac{|x_{n+1} - x_n|}{|x_n|} <$ tolerance
  \item $|f(x_{n+1})|<$ tolerance
\end{itemize}
Tolerance is usually $10^{-10}$.

\textbf{2.4 case 2 for solving nonlinear equations ($n = p < 1$)}\\

\textbf{2.4.1 Newton's or Tangent method}\\
We construct a recursive sequence with: $x_0$ is an initial term
\begin{center}
    \begin{math}
        x_{n+1} = x_n - \dfrac{f(x_n)}{f'(x_n)}
    \end{math}
\end{center}

\textbf{2.4.x Broyden's bethod}
It approximates $JF(x^{(m)})$ with some matrix $B_bn$
where:
\begin{center}
    \begin{math}
        \text{slope} \approx f'(x_1)
    \end{math}\\
    \begin{math}
        \dfrac{f(x_2) - f(x_1)} {x_2 - x_1} \approx f(x_1)
    \end{math}\\
    \begin{math}
        f(x_2) - f(x_1) \approx f'(x) (x_2 - x_1)
    \end{math}
\end{center}
Where $<f(x_2) - f(x_1)>$ is a vector, $<f(x_1)>$ is a $n \times n$ matrix $B_n$, and
$<(x_2 - x_1)>$ a vector.

\textbf{2.4.x Conclusion}
\begin{enumerate}
    \item Use Broyden's method first
    \item If it doesn't converge, go and compute derivatives for
        Newtons method.
    \begin{enumerate}
        \item Do a few steps of gradient descent(GD)
        \item Use the final approximate of GD as entry into Newton's method
    \end{enumerate}
\end{enumerate}

\section{\underline{Curves}}

\end{multicols}
\end{document}

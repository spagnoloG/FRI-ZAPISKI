\documentclass{article}
\usepackage[margin=0.15cm]{geometry}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\begin{document}

\begin{center}
	{\small OUI/FRI \par}
\end{center}


\begin{multicols}{2}

	\section{Osnove}

	\subsection{Kaj je umetna inteligenca?}
	\begin{itemize}
		\item  \textbf{cilji:} Razumeti in zgraditi inteligentne sisteme na osnovi razumevanja
		      cloveskega \textit{razmisljanja, sklepanja, ucenja} in \textit{komuniciranja}.
	\end{itemize}

	\section{Strojno Ucenje}

	\subsection{Kaj je stronjo ucenje?}
	Je \textit{podrocje umetne inteligence}, ki raziskuje kako se lahko algorimti samodejno izboljusjejo ob pridobivanju izkusenj.

	\subsection{Vrste ucenja:}

	\begin{itemize}
		\item \textbf{Nadzorovano ucenje} \textit{supervised learning}:
		      Ucni primeri so oznaceni in podani kot vrednosti vhodov in izhodov.
		      Ucimo se funkcije, ki vhode preslika v izhode. (npr. odlocitveno drevo)
		\item \textbf{Nenadzorovano ucenje} \textit{unsupervised learning}:
		      Ucni primeri niso oznaceni $\rightarrow$ nimajo ciljne spremenljivke. Ucimo
		      se iz vzorcev v podatkih. (npr. grucenje)
		\item \textbf{Spodbujevalno ucenje} \textit{reinforcment learning}:
		      Inteligentni agen se uci iz zaporedja nagrad in kazni.
	\end{itemize}

	\subsection{Nadzorovano ucenje}
	Podano imamo mnozico \textbf{ucnih} primerov:

	\begin{center}
		\begin{math}
			(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)
		\end{math}
	\end{center}
	kjer je vsak $y_j$ vrednost neznane funkcije $y = f(x)$.
	Nasa naloga je posikati hipoteticno funkcijo $h$, ki je najboljsi
	mozen priblizek funkciji $f$.

	Locimo dve vrsti problemov:
	\begin{enumerate}
		\item \textbf{Klasifikacijski}: $y_j$ je \textit{diskretna(kategoricna)} spremenljivka
		      \begin{itemize}
			      \item $y$ pripada \textbf{koncnemu naboru vrednosti} (diskretna spremenljivka)
			      \item $y$ imenujemo \textbf{razred} (class)
		      \end{itemize}
		\item \textbf{Regresijski}: $y_j$ je \textit{zvezna} spremenljivka
		      \begin{itemize}
			      \item $y$ je stevilo (obicajno $y \in R$, je zvezna spremenljivka)
			      \item $y$ imenujemo \textbf{oznacba} (label)
		      \end{itemize}
	\end{enumerate}

	\subsubsection{Prostor in evalviranje hipotez}
	Denimo da imamo:
	\begin{itemize}
		\item binarno klasifikacijo
		\item $n$ binarnih atributov
	\end{itemize}
	Iz tega sledi:
	\begin{itemize}
		\item $2^n$ razlicnih ucnih primerov
		\item $2^{2^n}$ hipotez (celotno odlocitveno drevo)
	\end{itemize}

	Pomembni kriteriji pri \textit{evalviranju} hipotez:
	\begin{itemize}
		\item \textbf{konsistentnost} hipotez s (ucnimi) primeri
		\item \textbf{splosnost} tocnost za nevidene primere
		\item \textbf{razumljivost} (\textit{interpretability, comprehensibility}) hipotez
	\end{itemize}

	Poznamo 4 razrede za ocenjevanje uspesnosti pri klasifikaciji na podlagi njihove \textbf{tocnosti}:
	\begin{itemize}
		\item \textbf{TP} - pravilno pozitivno klasificirani primeri
		\item \textbf{TN} - pravilno negativno klasificirani primeri
		\item \textbf{FP} - napacno pozitivno klasificirani primeri
		\item \textbf{FN} - napacno negiativno klasificirani primeri
	\end{itemize}

	\textbf{Klasifikacijska tocnost} je potem definirana:

	\begin{center}
		\begin{math}
			CA = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{N}
		\end{math}
	\end{center}

	Poznamo dva tipa atributov:

	\begin{enumerate}
		\item \textbf{diskretni} atributi:
		      \begin{itemize}
			      \item  \textbf{nominalni} npr. ['soncno', 'dezevno']
			      \item  \textbf{ordinalni} npr. ['nizko', 'srednje', 'visoko']
		      \end{itemize}
		      Odlocitvena drevesa delijo prvotno ucno mnozico na vse manjse podmnozice
		\item \textbf{zvezni} atributi:

		      Delitev podmnozice glede na smislno mejo izbranega atributa
	\end{enumerate}

	\subsubsection{Odlocitveno drevo}
	Ponazarja relacijo med vhodnimi \textit{vrednostmi/atributi} in \textit{odlocitvojo/ciljno} spremenljivko.

	Z \textbf{notranjimi vozlisci} opravljamo test glede na vrednost posameznega atributa. Na koncu pridemo do \textbf{lista}, ki nam
	s poroci odlocitev (vrednost ciljne spremenljivke). Konjunkcijo pogojev v \textit{notranjih vozliscih} katera vodi do \textit{lista}
	imenujemo \textbf{pot}.

	Gradnja odlocitvenega drevesa:
	Nas cilj je zgraditi \textbf{cim manjse drevo}, ki je \textbf{konsistentno} z ucnimi podatki.

	\textbf{Hevristicni pozrezsni algoritem - TDIDT} s strategijo \textbf{razveji in omeji}:
	\begin{itemize}
		\item Izberi najbolj pomemben atribut - tisti, ki najbolj odlocilno vpliva na klasifikacijo primera in razdeli
		      primere v poddrevesa glede na njegove vrednosti
		\item  rekurizvno ponovi za vsa drevesa
		\item ce vsi elementi v listu pripadajo istemu razredu ali vozlisca ni mozno deliti naprej(ni razpolozljivih atributov), ustavi gradnjo
	\end{itemize}

	\textbf{Kratovidnost} TDIDT:
	Ker je TDIDT pozresni algoritem, ki "lokalno" izbira najboljsi atribut, ne upsteva kako dobro drugi algoritmi
	doplnjujejo izbrani atribut.


	\paragraph{Izbor najbolj pomembnega atributa in informacijski prispevek}
	Najboljsi atribut je tisti, ki razdeli ucno mnozico v najbolj ciste podmnozice.
	Uporabimo lahko mero entropije:
	\begin{center}
		\begin{math}
			H(X) = \sum_{i=1}^{n} p_i I_i = -\sum_{i=1}^{n} p_i log_2 p_i
		\end{math}
	\end{center}

	Zanima nas \textbf{znizanje} entropije ( \textit{nedolocenosti} ) ob delitvi ucne mnozice glede
	na vrednosti atributa $A$.

	Definirajmo \textbf{informacijski prispevek} na taksen nacin,
	da najbolj informativni atribut \textbf{maksimizira informacijski prispevek} oz. minimizira $I_{res}$.
	\begin{center}
		\begin{math}
			Gain(A) = I - I_{res}(A)
		\end{math}

		\begin{math}
			I_{res} = - \sum_{v_i \in A} p_{v_i} \sum_c p(c|v_i) log_2 p(c|v_i)
		\end{math}
	\end{center}

	\paragraph{Vecvrednostni atributi}
	Tezava z atributi, ki imajo vec kot dve vrednosti: Informacijski prispevek precenjuje
	njihovo kakovost(entropija je visja na racun vecjega stevila vrednosti in ne na racun kakovosti atributa)

	resitve:
	\begin{itemize}
		\item normalizacija informacijskega prispevka: \textbf{relativni informacijski prispevek} ali IGR (information gain ratio)
		      \begin{center}
			      \begin{math}
				      Gain(a) = I - I_{res}(A)
			      \end{math},
			      \begin{math}
				      I(A) = - \sum_v p_v log_2 p_v
			      \end{math}
			      \begin{math}
				      GainRatio(A) = \frac{Gain(A)}{I(A)} = \frac{I - I_{res}(A)}{I(A)}
			      \end{math}
			      Oba zelimo maksimizirati
		      \end{center}

		\item uporaba \textbf{alternativnih mer}: npr. \textbf{Gini index}
		      \textit{Ocena pricakovane klasifikacijske napake, vsota produktov verjetnosti razredov}
		      \begin{center}
			      \begin{math}
				      Gini = \sum_{c1 \neq c2} p(c_1) p(c_2)
			      \end{math}
			      \begin{math}
				      Gini(A) = \sum_{v} p(v) \sum_{c1 \neq c2} p(c_1 | v) p(c_2 | v)
			      \end{math}
		      \end{center}
		\item \textbf{binarizacija} atributov:
		      Je alternativa za resevanje problematike z vecvrednostmi atributi.
		      Prednosti binarizacije so manjsa vejanja drevesa, kar je statisticno bolj zaneslijvo. Razlicni nacini
		      binarizacije atributa lahko nastopajo kot samostojni atributi, ki se v drevesu pojavijo veckrat.
	\end{itemize}

	\paragraph{Privzeta tocnost in Pristranost na ucni mnozici}

	Smiselna mera za \textbf{Privzeto tocnost} odlocitvenega drevesa je \textbf{verjetnost vecinskega razreda} v
	ucni mnozici. Drevo je uporabno, ce je njegova tocnost \textbf{visja} od privzete tocnosti.

	npr. [\#Da, \#Ne] = [3, 7] $\rightarrow$ verjetnost vecinskega razreda: 7/10


	Nas cilj je maksimizirati pricakovano tocnost na testnih podatkih vendar se zelimo izogniti pretiranemu prilaganju. Zato obicajno podatke
	razdelimo na \textbf{ucno} (70\%) in \textbf{testno} mnozico (30\%).

	\paragraph{Ucenje dreves iz sumnih podatkov}
	V primeru da podatki niso popolni(premalo primerov / atributov) ali napake
	se lahko pojavijo tezave:
	\begin{itemize}
		\item \textbf{Ucenje suma} in ne dejanske aproksimacijske funkcije
		\item Pretirano prilaganje vodi v \textbf{prevelika drevesa} $\rightarrow$ overfitting
		\item \textbf{Slaba razumljivost} dreves
	\end{itemize}
	Posledica: \textbf{nizja klasifikacijska tocnost} na novih nevidenih podatkih.

	\paragraph{Rezanje odlocitvenih dreves}
	Resujemo problem prevelikega prilaganja ucnim podatkom.
	Nizji deli drevesa predstavljajo vecje lokalno prilaganje ucnim podatkom, ki so lahko posledica suma.
	Poznamo dve strategiji rezanja dreves:
	\begin{enumerate}
		\item \textbf{Rezanje vnaprej}: uporaba dodatnega kriterija za zaustavitev
		      gradnje devesa glede na obseg suma. Je hitrejsi pristop, vendar kratkoviden (uposteva samo zgornji del drevesa)
		\item \textbf{Rezanje nazaj}: Rezanje, ki po gradnji celotnega drevesa, odstrani manj zanesljive
		      dele drevesa(opisujejo sum, zgrajeni iz manj podatkov in z manj informirani atributi)
		      Je pocasneje, in upostecva informacijo iz \textbf{celega drevesa}.
	\end{enumerate}

	\begin{enumerate}
		\item \textbf{Rezanje z zmanjsevanjem napake (REP)}

		      Uporablja posebno reazalno (validacijsko) mnozico:
		      \begin{itemize}
			      \item \textbf{Ucna mnozica} (70\%):
			            \begin{itemize}
				            \item mnozica za gradnjo \textit{growing set} (70\%)
				            \item rezalna mnozica \textit{prunning set} (30\%)
			            \end{itemize}
			      \item \textbf{Testna mnozica} (30\%)
		      \end{itemize}
		      Postopek:
		      \begin{enumerate}
			      \item Potuj od listov navzgor(pricni s starsi listov)
			      \item Za vsako vozlisce \textbf{v} izracunaj \textbf{dobitek rezanja}

			            Dobitek rezanja izracunamo: st. napacnih klasifikacij v drevesu \textbf{T} - st. napacnih klasifikacij v vozliscu \textbf{v}.
			      \item Ce je dobitek $\geq 0$, obrezi in nadaljuj postopek s starsem, sicer ustavi postopek
		      \end{enumerate}
		\item \textbf{Rezanje z minimizacijo napake (MEP)}

		      Uporablja mnozico za gradnjo drevesa in ne locene rezalne mnozice.
		      Postopek: Za vozlisce \textbf{v} izracunamo:
		      \begin{enumerate}
			      \item \textbf{staticno napako} (verjetnost klasifikacije v napacen razred)
			            \begin{center}
				            \begin{math}
					            e(v) = p(razred \neq C | v)
				            \end{math},
				            C je vecinski razred v \textbf{v}
			            \end{center}
			      \item \textbf{vzvratno napako} (\textit{backed-up error})
			            \begin{center}
				            \begin{math}
					            \sum_i p_i E(T_i) = p_1 E(T_1) + p_2 E(T_2) + \dots
				            \end{math}
			            \end{center}
		      \end{enumerate}
		      Rezemo, ce je \textbf{staticna napaka} $<$ \textbf{vzvratna napaka}.

		      Napaka \textbf{optimalno} obrezanega drevesa je torej:
		      \begin{center}
			      \begin{math}
				      E(T) = min(e(v), \sum_i p_i E(T_i))
			      \end{math} in
			      \begin{math}
				      E(T) = e(v)
			      \end{math}, ce je \textbf{v} list.
		      \end{center}

		      Namesto \textbf{minimizacije} napake E, lahko provblem obrnemo in \textbf{maksimiziramo} tocnost CA.
	\end{enumerate}

	\paragraph{Ocenjevanje verjetnosti}

	\begin{enumerate}
		\item \textbf{Laplaceova ocena verjetnosti}
		      \begin{center}
			      \begin{math}
				      p = \frac{n+1}{N+k}
			      \end{math}
		      \end{center}

		      n - st. primerov, ki pripadajo razredu C\\
		      N - st. vseh primerov\\
		      k - st. vseh razredov\\
		      $k$ je problematicen parameter, saj ocena ne uposetva apriorne verjetnosti

		\item \textbf{m-ocena verjetnosti}
		      \begin{center}
			      \begin{math}
				      p = \frac{n+ p_a m}{N+m} = p_a \frac{m}{N+m} + \frac{n}{N}\frac{N}{N + m}
			      \end{math}
		      \end{center}

		      $p_a$ - Apriorna verjetnost razreda C\\
		      $m$ - parameter ocene (vpliva na delez upostevanja apriorne verjetnosti)\\
		      Ce imamo malo suma, potem $m$ nastavimo na majno vrednost in imamo malo rezanja. Obratno v primeru ce imamo veliko suma.
		      Gre se za posplositev Laplaceove ocene za $m=k$ in $p_a = 1/k$.
	\end{enumerate}

\end{multicols}
\end{document}
